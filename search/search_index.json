{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Center Fabric Blueprint # Introduction # Welcome to the Nokia Data Center Blueprint. The goal of this blueprint is to review many of the important design considerations when building a data center fabric and provide some suggestions. This will be a living document so new sections will be added over time. Our initial focus will be on deploying SR Linux for a routed data center fabric. Topics like building a lab with SR Linux software and using EVPN & VXLAN for network virtualization will be added soon. This blueprint isn\u2019t meant as a replacement for the full set of User Guides published with each release of SR Linux software. The User Guide provide great detail for all of the supported features of SR Linux. Who should read this? # The target audience is a network architect or SRE (Site Reliability Engineer) looking build a data center fabric using Nokia SR Linux and IXR switches. Nokia Data Center Fabric Solution # There are three major components of the Nokia Data Center Fabric solution. A brief introduction is provided here and more information can be found at https://www.nokia.com/networks/dc-fabric/ SR Linux # Nokia SR Linux is a new NOS developed specifically to meet the needs of hyper-scalers and yet be consumable by any customer regardless of size. Nokia SR Linux opens up the future for cloud builder and hyper-scaler data center networking teams. Its model-driven foundation is designed from the ground up with a management architecture that meets today\u2019s demands for scalability, visibility, cost-efficiency and ease of operations. SR Linux achieves true openness and extensibility by using an unmodified Linux-based kernel as the foundation on which to build and run a suite of network applications. This gives network teams reliability, portability and ease of application development \u2014 while also speeding the availability of non-Nokia applications. -- https://www.nokia.com/networks/dc-fabric/simplify/ IXR Hardware # SR Linux is designed to run on the IXR family of data center switches. Our [hardware] portfolio is based on merchant silicon with a common hardware design. Products include the Nokia 7220 Interconnect Router (IXR) and Nokia 7250 IXR, offering a broad range of high-performance platforms for data center top of rack (TOR), leaf, spine and super-spine applications. Choose between fixed-form-factor and chassis-based platforms, so your data center network teams can select the appropriate hardware while still enjoying all the benefits of running the same SR Linux NOS. -- https://www.nokia.com/networks/dc-fabric/routers/ Fabric Services System # The Nokia Fabric Services Platform is designed for the intent-based automation of all phases of data center fabric operations, giving your data center networking team all the confidence, efficiency, agility and control they crave. Based on an open Kubernetes framework, all fabric services use a distributed microservices approach to deliver a true cloud-native automation and operations platform. The platform also provides a digital sandbox that is a true emulation of the data center fabric, creating a virtual digital twin of the live production network. This emulates a data center fabric and application workloads and can emulate Day 0 design, Day 1 deployment and Day 2+ operations such as operation, fabric change management and troubleshooting. -- https://www.nokia.com/networks/dc-fabric/automate/","title":"Introduction"},{"location":"#data-center-fabric-blueprint","text":"","title":"Data Center Fabric Blueprint"},{"location":"#introduction","text":"Welcome to the Nokia Data Center Blueprint. The goal of this blueprint is to review many of the important design considerations when building a data center fabric and provide some suggestions. This will be a living document so new sections will be added over time. Our initial focus will be on deploying SR Linux for a routed data center fabric. Topics like building a lab with SR Linux software and using EVPN & VXLAN for network virtualization will be added soon. This blueprint isn\u2019t meant as a replacement for the full set of User Guides published with each release of SR Linux software. The User Guide provide great detail for all of the supported features of SR Linux.","title":"Introduction"},{"location":"#who-should-read-this","text":"The target audience is a network architect or SRE (Site Reliability Engineer) looking build a data center fabric using Nokia SR Linux and IXR switches.","title":"Who should read this?"},{"location":"#nokia-data-center-fabric-solution","text":"There are three major components of the Nokia Data Center Fabric solution. A brief introduction is provided here and more information can be found at https://www.nokia.com/networks/dc-fabric/","title":"Nokia Data Center Fabric Solution"},{"location":"#sr-linux","text":"Nokia SR Linux is a new NOS developed specifically to meet the needs of hyper-scalers and yet be consumable by any customer regardless of size. Nokia SR Linux opens up the future for cloud builder and hyper-scaler data center networking teams. Its model-driven foundation is designed from the ground up with a management architecture that meets today\u2019s demands for scalability, visibility, cost-efficiency and ease of operations. SR Linux achieves true openness and extensibility by using an unmodified Linux-based kernel as the foundation on which to build and run a suite of network applications. This gives network teams reliability, portability and ease of application development \u2014 while also speeding the availability of non-Nokia applications. -- https://www.nokia.com/networks/dc-fabric/simplify/","title":"SR Linux"},{"location":"#ixr-hardware","text":"SR Linux is designed to run on the IXR family of data center switches. Our [hardware] portfolio is based on merchant silicon with a common hardware design. Products include the Nokia 7220 Interconnect Router (IXR) and Nokia 7250 IXR, offering a broad range of high-performance platforms for data center top of rack (TOR), leaf, spine and super-spine applications. Choose between fixed-form-factor and chassis-based platforms, so your data center network teams can select the appropriate hardware while still enjoying all the benefits of running the same SR Linux NOS. -- https://www.nokia.com/networks/dc-fabric/routers/","title":"IXR Hardware"},{"location":"#fabric-services-system","text":"The Nokia Fabric Services Platform is designed for the intent-based automation of all phases of data center fabric operations, giving your data center networking team all the confidence, efficiency, agility and control they crave. Based on an open Kubernetes framework, all fabric services use a distributed microservices approach to deliver a true cloud-native automation and operations platform. The platform also provides a digital sandbox that is a true emulation of the data center fabric, creating a virtual digital twin of the live production network. This emulates a data center fabric and application workloads and can emulate Day 0 design, Day 1 deployment and Day 2+ operations such as operation, fabric change management and troubleshooting. -- https://www.nokia.com/networks/dc-fabric/automate/","title":"Fabric Services System"},{"location":"fabric/fabric-considerations/","text":"Overall Considerations # When designing a Clos fabric, there are a number of parameters that need to be taken into consideration ranging from the physical layer to the service layer to the application layer. Here are some questions for consideration: Server density per rack and connection speed Power budgets per rack, per POD, per DC Distance between racks Existing cable infrastructure What applications will run in the fabric Applications that are sensitive to latency and/or packet loss (may determine the physical proximity and/or oversubscription ratio) Compute virtualization or containers/micro-services How much bandwidth is required per server How much traffic is required between servers How is storage being connected (Fiber Channel, iSCSI, NFS) While this isn't an exhaustive list, answers to these questions combined with an understanding of growth plans will allow a proper network to be designed. Leaf Considerations # In a Clos network fabric, leaf (ToR) switches are the devices connecting to the end devices. These end devices are most commonly servers, but could also be security devices, load-balancers or other routers. Depending on the end device type, the leaf switch may have additional network roles. In a fabric supporting overlay networks (e.g. EVPN/VXLAN), the leaf switch will commonly be a tunnel end point (TEP). In the case of VXLAN the leaf switch could be responsible for encap/decap of VXLAN headers. Whether or not a leaf will be required to be TEP can impact what type of physical switch can used (most but not all network chipsets support this function). Depending on the level of redundancy required by the attached servers, there often two leaf switches installed in each server rack. Advantages of having two leaf switches include protection against switch failure, replacement or upgrade. From the perspective of the server connecting to the leaf switches it is important that logically the switches appear as a single device and the server can attach via a standard 802.3ad trunk. On the switch side, Nokia has uses the multi-homing capabilities within EVPN to properly forward traffic and prevent network loops. While not widespread yet, it is also possible to route traffic all the way to the end device. This configuration would not require a 802.3ad trunk but usually relies on a routing protocol on the server that peers with the leaf switch. Server connectivity is discussed in more detail in a later section of this blueprint. A leaf switch is connecting network devices rather than compute or storage is often called a \"border leaf.\" In the border leaf use case it is also common for the switch to act as a TEP, so selecting the right switch platform is an important consideration. Other factors such as tunnel scale, route table size and buffers also impact the platform chosen for a border leaf. Spine Considerations # Spine switches are responsible for connecting all of the leaf switches. In the case where a superspine tier is used, spine switches will also connect to one or more of the superspines (depending on the type of design). Because of this role, it is common to avoid oversubscription. Oversubscription should be moved to the edges of the network, at the leaf or border leaf with platforms suitable to supporting this requirement. Based on the size of the network it is also common to consider both fixed-form and chassis switches. In the last few years, there has been a preference for fixed-form switches to reduce cost as well as complexity. If buffers are not needed nor TEP functions then spine requirements are quite simple; focused primarily on speed and cost. As the speed of network ASICs increaases, the network diameter supported by a single ASIC also increases. For example, a Broadcom Tomahawk3 ASIC supports up to 128 100G ports on a single chip. This means a simple spine/leaf fabric with Tomahawk3 at the spine could support up to 128 leaf switches. Factor in multi-stage designs and it is easy to see how the largest networks can use these platforms to scale ecoonomically. Next, let's consider two example networks.","title":"Considerations"},{"location":"fabric/fabric-considerations/#overall-considerations","text":"When designing a Clos fabric, there are a number of parameters that need to be taken into consideration ranging from the physical layer to the service layer to the application layer. Here are some questions for consideration: Server density per rack and connection speed Power budgets per rack, per POD, per DC Distance between racks Existing cable infrastructure What applications will run in the fabric Applications that are sensitive to latency and/or packet loss (may determine the physical proximity and/or oversubscription ratio) Compute virtualization or containers/micro-services How much bandwidth is required per server How much traffic is required between servers How is storage being connected (Fiber Channel, iSCSI, NFS) While this isn't an exhaustive list, answers to these questions combined with an understanding of growth plans will allow a proper network to be designed.","title":"Overall Considerations"},{"location":"fabric/fabric-considerations/#leaf-considerations","text":"In a Clos network fabric, leaf (ToR) switches are the devices connecting to the end devices. These end devices are most commonly servers, but could also be security devices, load-balancers or other routers. Depending on the end device type, the leaf switch may have additional network roles. In a fabric supporting overlay networks (e.g. EVPN/VXLAN), the leaf switch will commonly be a tunnel end point (TEP). In the case of VXLAN the leaf switch could be responsible for encap/decap of VXLAN headers. Whether or not a leaf will be required to be TEP can impact what type of physical switch can used (most but not all network chipsets support this function). Depending on the level of redundancy required by the attached servers, there often two leaf switches installed in each server rack. Advantages of having two leaf switches include protection against switch failure, replacement or upgrade. From the perspective of the server connecting to the leaf switches it is important that logically the switches appear as a single device and the server can attach via a standard 802.3ad trunk. On the switch side, Nokia has uses the multi-homing capabilities within EVPN to properly forward traffic and prevent network loops. While not widespread yet, it is also possible to route traffic all the way to the end device. This configuration would not require a 802.3ad trunk but usually relies on a routing protocol on the server that peers with the leaf switch. Server connectivity is discussed in more detail in a later section of this blueprint. A leaf switch is connecting network devices rather than compute or storage is often called a \"border leaf.\" In the border leaf use case it is also common for the switch to act as a TEP, so selecting the right switch platform is an important consideration. Other factors such as tunnel scale, route table size and buffers also impact the platform chosen for a border leaf.","title":"Leaf Considerations"},{"location":"fabric/fabric-considerations/#spine-considerations","text":"Spine switches are responsible for connecting all of the leaf switches. In the case where a superspine tier is used, spine switches will also connect to one or more of the superspines (depending on the type of design). Because of this role, it is common to avoid oversubscription. Oversubscription should be moved to the edges of the network, at the leaf or border leaf with platforms suitable to supporting this requirement. Based on the size of the network it is also common to consider both fixed-form and chassis switches. In the last few years, there has been a preference for fixed-form switches to reduce cost as well as complexity. If buffers are not needed nor TEP functions then spine requirements are quite simple; focused primarily on speed and cost. As the speed of network ASICs increaases, the network diameter supported by a single ASIC also increases. For example, a Broadcom Tomahawk3 ASIC supports up to 128 100G ports on a single chip. This means a simple spine/leaf fabric with Tomahawk3 at the spine could support up to 128 leaf switches. Factor in multi-stage designs and it is easy to see how the largest networks can use these platforms to scale ecoonomically. Next, let's consider two example networks.","title":"Spine Considerations"},{"location":"fabric/fabric-intro/","text":"Fabric Design Considerations # As we look to implement best practices for building data center networks, it is important to realize there is a not a \"one size fits all\" answer. While there are certainly some best practices to follow, each network architect must understand their own environment and implement accordingly. That said, there are clear benefits to moving away from legacy network designs that were designed primarily using large Layer 2 domains and supporting North-to-South (human-to-machine) traffic patterns. The rise of micro-services applications and East-to-West (machine-to-machine) traffic patterns is forcing the network to be re-designed to meet these new demands. Background # Legacy data center designs were generally composed of three-tiers - access, aggregation and core (see Figure 2-1 ). Servers (usually bare metal or early virtualized hosts) connected to the access layer. The aggregation and core layers were often large chassis-based platforms. Layer 2 and Layer 3 traffic was segregated at the aggregation layer. Figure 2-1 This design has several inherent disadvantages: Scalability of the aggregation and core layers Sub-optimal and non-deterministic traffic paths Inter-subnet traffic tromboning via aggregation and/or core The use of xSTP as a Layer 2 anti-loop mechanism blocked links and left them unused Evolutionary techniques such as MC-LAG helped to reduce the need for spanning-tree and provide server access redundancy (see Figure 2-2 ) but the other disadvantages remain. Some vendors and standards bodies pursued solutions such as TRILL and SPB but these efforts did not gain significant market adoption. In the end, as in many other instances, rather than creating a new protocol the market fell back on IP routing and standard routing protocols such as BGP. Figure 2-2 For several years, the largest network providers have been building scale-out networks using small, fixed-configuration switches. These switches are arranged according to Clos design principles. Clos networks are named after one of the scientists who popularized this method, Charles Clos, and it originally defined an efficient way of building high-capacity, scale-out circuit switching networks. Modern data centers can use these same principles to build Layer 3, routed networks (see Figure 2-3 ). Networks (or Fabrics) designed in this way, overcome all of the limitations listed above with one drawback - Layer 2 domains are now confined to a individual (or pair) of access (also called Leaf or Top of Rack (ToR)) switches. To support Layer 2 domains beyond this limit, network virtualization technologies such as VXLAN and EVPN can be used. This is an important topic and will be discussed in a later section. Figure 2-3 A simple Clos network is comprised of leaf switches and spine switches (See Figure 2-3 ). The leaf (or ToR) switches connect to the end devices/hosts and are connected together via the spine switches. Every leaf switch is connected to every spine and thereby is at most one \"hop\" away from any other leaf in the network. These leaf/spine networks can also become a building block of larger networks by implementing another spine layer (sometimes called a \"superspine\"). Multi-stage Clos networks are quite common, especially in large data centers. Section 3.2 of RFC 7938 provides more information about Clos topologies for the data center. Facebook , Dropbox and others have written extensively on their network design.","title":"Introduction"},{"location":"fabric/fabric-intro/#fabric-design-considerations","text":"As we look to implement best practices for building data center networks, it is important to realize there is a not a \"one size fits all\" answer. While there are certainly some best practices to follow, each network architect must understand their own environment and implement accordingly. That said, there are clear benefits to moving away from legacy network designs that were designed primarily using large Layer 2 domains and supporting North-to-South (human-to-machine) traffic patterns. The rise of micro-services applications and East-to-West (machine-to-machine) traffic patterns is forcing the network to be re-designed to meet these new demands.","title":"Fabric Design Considerations"},{"location":"fabric/fabric-intro/#background","text":"Legacy data center designs were generally composed of three-tiers - access, aggregation and core (see Figure 2-1 ). Servers (usually bare metal or early virtualized hosts) connected to the access layer. The aggregation and core layers were often large chassis-based platforms. Layer 2 and Layer 3 traffic was segregated at the aggregation layer. Figure 2-1 This design has several inherent disadvantages: Scalability of the aggregation and core layers Sub-optimal and non-deterministic traffic paths Inter-subnet traffic tromboning via aggregation and/or core The use of xSTP as a Layer 2 anti-loop mechanism blocked links and left them unused Evolutionary techniques such as MC-LAG helped to reduce the need for spanning-tree and provide server access redundancy (see Figure 2-2 ) but the other disadvantages remain. Some vendors and standards bodies pursued solutions such as TRILL and SPB but these efforts did not gain significant market adoption. In the end, as in many other instances, rather than creating a new protocol the market fell back on IP routing and standard routing protocols such as BGP. Figure 2-2 For several years, the largest network providers have been building scale-out networks using small, fixed-configuration switches. These switches are arranged according to Clos design principles. Clos networks are named after one of the scientists who popularized this method, Charles Clos, and it originally defined an efficient way of building high-capacity, scale-out circuit switching networks. Modern data centers can use these same principles to build Layer 3, routed networks (see Figure 2-3 ). Networks (or Fabrics) designed in this way, overcome all of the limitations listed above with one drawback - Layer 2 domains are now confined to a individual (or pair) of access (also called Leaf or Top of Rack (ToR)) switches. To support Layer 2 domains beyond this limit, network virtualization technologies such as VXLAN and EVPN can be used. This is an important topic and will be discussed in a later section. Figure 2-3 A simple Clos network is comprised of leaf switches and spine switches (See Figure 2-3 ). The leaf (or ToR) switches connect to the end devices/hosts and are connected together via the spine switches. Every leaf switch is connected to every spine and thereby is at most one \"hop\" away from any other leaf in the network. These leaf/spine networks can also become a building block of larger networks by implementing another spine layer (sometimes called a \"superspine\"). Multi-stage Clos networks are quite common, especially in large data centers. Section 3.2 of RFC 7938 provides more information about Clos topologies for the data center. Facebook , Dropbox and others have written extensively on their network design.","title":"Background"},{"location":"fabric/examples/example-3stage/","text":"3-Stage Clos Example # Overview # A 3-stage Clos and a 5-stage Clos are provided as examples. These designs provide the basic concepts and can be extended if needed. For simplicity, both examples are built with 7220-IXR-D series switches which are built on the Trident3 chipset. These examples are not exhaustive. There are many different ways to build Clos networks depending on the design goals. Network Diagram # 3-stage Clos Assumptions # Uplinks from Leaf to Spine are 100G Servers can connect at either 10G or 25G Support for single-homed or dual-homed servers (only impact is server density) Oversubscription calculations are based on 25G downlink and 100G uplink Ratio could change by adding more Spines (up to 8 max) Connections out of the fabric will come via a pair of Leaf nodes (Border Leafs) Oversubscription ratio is 1:1 between Spine and Border Leaf Details # Spine node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) Leaf node 7220-IXR-D2 (48xSFP28+8xQSFP28) Border Leaf node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) # of Spines 4 # of Leafs 30 # of Border Leafs 2 Max single-homed servers 1,440 (48*30) Max dual-homed servers 720 (48*15) Leaf-to-spine oversubscription 3:1","title":"3-stage Clos"},{"location":"fabric/examples/example-3stage/#3-stage-clos-example","text":"","title":"3-Stage Clos Example"},{"location":"fabric/examples/example-3stage/#overview","text":"A 3-stage Clos and a 5-stage Clos are provided as examples. These designs provide the basic concepts and can be extended if needed. For simplicity, both examples are built with 7220-IXR-D series switches which are built on the Trident3 chipset. These examples are not exhaustive. There are many different ways to build Clos networks depending on the design goals.","title":"Overview"},{"location":"fabric/examples/example-3stage/#network-diagram","text":"3-stage Clos","title":"Network Diagram"},{"location":"fabric/examples/example-3stage/#assumptions","text":"Uplinks from Leaf to Spine are 100G Servers can connect at either 10G or 25G Support for single-homed or dual-homed servers (only impact is server density) Oversubscription calculations are based on 25G downlink and 100G uplink Ratio could change by adding more Spines (up to 8 max) Connections out of the fabric will come via a pair of Leaf nodes (Border Leafs) Oversubscription ratio is 1:1 between Spine and Border Leaf","title":"Assumptions"},{"location":"fabric/examples/example-3stage/#details","text":"Spine node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) Leaf node 7220-IXR-D2 (48xSFP28+8xQSFP28) Border Leaf node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) # of Spines 4 # of Leafs 30 # of Border Leafs 2 Max single-homed servers 1,440 (48*30) Max dual-homed servers 720 (48*15) Leaf-to-spine oversubscription 3:1","title":"Details"},{"location":"fabric/examples/example-5stage/","text":"5-stage Clos Example # Network Diagram # 5-stage Clos Assumptions # Uplinks from Leaf to Spine are 100G Servers can connect at either 10G or 25G Support for single-homed or dual-homed servers (only impact is server density) Leaf to Spine oversubscription calculations are based on 25G downlink and 100G uplink Connections out of the fabric will come via a pair of Leaf nodes (Border Leafs) Oversubscription ratio is 1:1 between Spine, Superspine and Border Leaf Number of Border Leafs can scale out based on amount of North/South traffic Details # Spine node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) Leaf node 7220-IXR-D2 (48xSFP28+8xQSFP28) Border Leaf node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) # of Spines per Pod 4 # of Leafs per Pod 16 # of Pods 4 # of Superspines 16 # of Border Leafs 2 (or more) Max single-homed servers 3,072 (48*64) Max dual-homed servers 1,536 (48*32) Leaf-to-spine oversubscription 3:1","title":"5-stage Clos"},{"location":"fabric/examples/example-5stage/#5-stage-clos-example","text":"","title":"5-stage Clos Example"},{"location":"fabric/examples/example-5stage/#network-diagram","text":"5-stage Clos","title":"Network Diagram"},{"location":"fabric/examples/example-5stage/#assumptions","text":"Uplinks from Leaf to Spine are 100G Servers can connect at either 10G or 25G Support for single-homed or dual-homed servers (only impact is server density) Leaf to Spine oversubscription calculations are based on 25G downlink and 100G uplink Connections out of the fabric will come via a pair of Leaf nodes (Border Leafs) Oversubscription ratio is 1:1 between Spine, Superspine and Border Leaf Number of Border Leafs can scale out based on amount of North/South traffic","title":"Assumptions"},{"location":"fabric/examples/example-5stage/#details","text":"Spine node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) Leaf node 7220-IXR-D2 (48xSFP28+8xQSFP28) Border Leaf node 7220-IXR-D3 (32xQSFP28 + 2xSFP+) # of Spines per Pod 4 # of Leafs per Pod 16 # of Pods 4 # of Superspines 16 # of Border Leafs 2 (or more) Max single-homed servers 3,072 (48*64) Max dual-homed servers 1,536 (48*32) Leaf-to-spine oversubscription 3:1","title":"Details"},{"location":"mgmt/mgmt-cli/","text":"Fabric Management # CLI # Overview # SR Linux provides a transaction-based CLI written in Python. The CLI reads directly from the YANG data models and provides the same information available via gNMI or JSON-RPC. Being transaction-based, the CLI allows for a number of changes to be made to the configuration before requiring an explicit commit from the operator before applying it. The default location for the configuration file is /etc/opt/srlinux/config.json . Upon boot up this config file is loaded by SR Linux mgmt_server when it starts and publishes content to IDB for other applications to consume. Warning The CLI should not be the primary method for automation or scripting. Rather it is suggested to use either the gNMI or JSON-RPC interfaces. If there is no configuration file present, a basic configuration file is auto-generated with the following values: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created, most in memory \u2013 one catch all in a file called messages SSH server is enabled Some default IPv4/v6 CPM filters Datastores # SR Linux has three datastores that are exposed as modes within the CLI. Running: the default mode when logging in and displays the intended configuration currently active on the device State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: without any modification this datastore is equivalent to the running datastore. Any changes made to this datastore require a commit before the changes go through YANG and application validation and then are applied to the device. This datastore has two modes currently: Shared Candidate: this is the default mode when entering the candidate mode in the CLI. This allows multiple users on the device to modify the same configuration concurrently and therefor when any of the users perform a commit it applies ALL changes from all users in the shared candidate mode. Exclusive Candidate: this mode must be explicitly specified when entering the candidate datastore. If an operator chooses to start an exclusive candidate session no other users may enter the candidate datastore to make changes. If there is an active shared candidate (uncommitted change in shared candidate) then the user is not allowed to enter in an exclusive mode. Likewise if there is an active exclusive candidate. A user can clear an active session using a \u2018tools\u2019 command. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device. Validation # When a configuration is committed from any of the configuration modes, the configuration goes through three sets of validation: YANG validation. This validation confirms parent/child syntax is present and that any references exist. YANG validation is not state or resource aware and therefore limited, however, it does provide a first level of validation (it is also the most performant type of validation \u2013 so is used in all cases where available). Any validation failure will result in the commit not being accepted and no configuration will be applied. Application validation. Each application receives a copy of the intended configuration and performs a test apply of the configuration. Each application will do its own validation and will not impact current system operation. If application validation fails, the commit fails and no configuration will be applied. Forwarding plane validation. If the forwarding plane is unable to accept the configuration then a graceful roll-back is done immediately. Examples are exhaustion of resources or the hardware cannot be programmed \u2013 this will result in a commit failure. Checkpoints # A configuration checkpoint can be created to save state at a particular time. It is then possible to revert back to a given checkpoint if needed. Checkpoints are created two different ways. First, using tools system configuration generate-checkpoint or second, using save checkpoint from candidate mode. It is also possible to set /system/configuration/auto-checkpoint to true which will generate a checkpoint whenever configuration is committed. Configuration can be reverted or rolled back to a previous checkpoing using either a tools or load command. Here are some additional notes regarding the checkpoint functionality: SRLinux allows for 10 checkpoints to be created by default but may be configured to save more up to 255 checkpoints. If the max-checkpoints value is changed to anything lower than the default 10 and there are existing 10+ checkpoints saved, the system will delete the older checkpoints required to meet new max-checkpoints value. When an operator creates a checkpoint, they may specify the following: Name of the checkpoint Comment to be added to a checkpoint When a checkpoint is created the following fields are also stored above and beyond the configured parameters: Created date Release running when created Username of user who created it Size of the checkpoint ID: checkpoints are 0 indexed with 0 being the newest, any new checkpoint created increases the index of all other checkpoints by 1 Features # Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands. Plugins # add a section or reference to plugins?","title":"CLI"},{"location":"mgmt/mgmt-cli/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-cli/#cli","text":"","title":"CLI"},{"location":"mgmt/mgmt-cli/#overview","text":"SR Linux provides a transaction-based CLI written in Python. The CLI reads directly from the YANG data models and provides the same information available via gNMI or JSON-RPC. Being transaction-based, the CLI allows for a number of changes to be made to the configuration before requiring an explicit commit from the operator before applying it. The default location for the configuration file is /etc/opt/srlinux/config.json . Upon boot up this config file is loaded by SR Linux mgmt_server when it starts and publishes content to IDB for other applications to consume. Warning The CLI should not be the primary method for automation or scripting. Rather it is suggested to use either the gNMI or JSON-RPC interfaces. If there is no configuration file present, a basic configuration file is auto-generated with the following values: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created, most in memory \u2013 one catch all in a file called messages SSH server is enabled Some default IPv4/v6 CPM filters","title":"Overview"},{"location":"mgmt/mgmt-cli/#datastores","text":"SR Linux has three datastores that are exposed as modes within the CLI. Running: the default mode when logging in and displays the intended configuration currently active on the device State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: without any modification this datastore is equivalent to the running datastore. Any changes made to this datastore require a commit before the changes go through YANG and application validation and then are applied to the device. This datastore has two modes currently: Shared Candidate: this is the default mode when entering the candidate mode in the CLI. This allows multiple users on the device to modify the same configuration concurrently and therefor when any of the users perform a commit it applies ALL changes from all users in the shared candidate mode. Exclusive Candidate: this mode must be explicitly specified when entering the candidate datastore. If an operator chooses to start an exclusive candidate session no other users may enter the candidate datastore to make changes. If there is an active shared candidate (uncommitted change in shared candidate) then the user is not allowed to enter in an exclusive mode. Likewise if there is an active exclusive candidate. A user can clear an active session using a \u2018tools\u2019 command. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.","title":"Datastores"},{"location":"mgmt/mgmt-cli/#validation","text":"When a configuration is committed from any of the configuration modes, the configuration goes through three sets of validation: YANG validation. This validation confirms parent/child syntax is present and that any references exist. YANG validation is not state or resource aware and therefore limited, however, it does provide a first level of validation (it is also the most performant type of validation \u2013 so is used in all cases where available). Any validation failure will result in the commit not being accepted and no configuration will be applied. Application validation. Each application receives a copy of the intended configuration and performs a test apply of the configuration. Each application will do its own validation and will not impact current system operation. If application validation fails, the commit fails and no configuration will be applied. Forwarding plane validation. If the forwarding plane is unable to accept the configuration then a graceful roll-back is done immediately. Examples are exhaustion of resources or the hardware cannot be programmed \u2013 this will result in a commit failure.","title":"Validation"},{"location":"mgmt/mgmt-cli/#checkpoints","text":"A configuration checkpoint can be created to save state at a particular time. It is then possible to revert back to a given checkpoint if needed. Checkpoints are created two different ways. First, using tools system configuration generate-checkpoint or second, using save checkpoint from candidate mode. It is also possible to set /system/configuration/auto-checkpoint to true which will generate a checkpoint whenever configuration is committed. Configuration can be reverted or rolled back to a previous checkpoing using either a tools or load command. Here are some additional notes regarding the checkpoint functionality: SRLinux allows for 10 checkpoints to be created by default but may be configured to save more up to 255 checkpoints. If the max-checkpoints value is changed to anything lower than the default 10 and there are existing 10+ checkpoints saved, the system will delete the older checkpoints required to meet new max-checkpoints value. When an operator creates a checkpoint, they may specify the following: Name of the checkpoint Comment to be added to a checkpoint When a checkpoint is created the following fields are also stored above and beyond the configured parameters: Created date Release running when created Username of user who created it Size of the checkpoint ID: checkpoints are 0 indexed with 0 being the newest, any new checkpoint created increases the index of all other checkpoints by 1","title":"Checkpoints"},{"location":"mgmt/mgmt-cli/#features","text":"Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.","title":"Features"},{"location":"mgmt/mgmt-cli/#plugins","text":"add a section or reference to plugins?","title":"Plugins"},{"location":"mgmt/mgmt-gnmi/","text":"Fabric Management # gNMI # Overview # gRPC Network Management Interface (gNMI) is a gRPC -based protocol that defines a service or set of services (set of RPC methods) used to configure and retrieve data from network devices. The gNMI gRPC service is comprised of four RPCs: CAPABILITIES, GET, SET and SUBSCRIBE. The gNMI spec and associated protobuf defines the type of messages and the data structures used to get, set or stream information to and from network devices. Method Description CAPABILITIES Retrieve the set of capabilities that is supported by the server. This allows the client to validate the service version that is implemented and retrieve the set of models that the server supports. The models can then be specified in subsequent RPCs to restrict the set of data that is utilized. GET Retrieve a snapshot of data from the server. A Get RPC requests that the server snapshots a subset of the data tree as specified by the paths included in the message and serializes this to be returned to the client using the specified encoding. SET Modify the state of data on the server. The paths to modified along with the new values that the client wishes to set the value to. SUBSCRIBE Request the server to send it values of particular paths within the data tree. These values may be streamed at a particular cadence (STREAM), sent one off on a long-lived channel (POLL), or sent as a one-off retrieval (ONCE). gNMI Subscription # A gNMI Subscription to a server consists of the following key attributes: One or multiple paths to subscribe to (what data should be sent to client by server) A subscription mode gNMI SUBSCRIBE RPC defines three subscription modes: ONCE: A subscription to a server is created and a single request is sent. The server creates an update message for the requested path and sends it to the client. The subscription is then terminated. POLL: A subscription to a server is created for a given path, the client will then send a specific message to the server on a timed interval, upon receipt of this message the server will send the data for the path on which the subscription is created. STREAM: A subscription to a server is created for a given path, the server is then expected to send data back to the client for that given path in one of three ways, on_change , on_sample or target_defined gNMI Server Configuration # -- { + running } -- [ system gnmi-server ] -- A:srl2# info detail admin-state enable timeout 7200 rate-limit 60 session-limit 20 commit-confirmed-timeout 0 include-defaults-in-config-only-responses false network-instance mgmt { admin-state enable use-authentication true port 57400 tls-profile tls-profile-1 } unix-socket { admin-state disable use-authentication true } Configuration explanation: admin-state : enabled under the global gNMI context it will allow for gNMI to be enabled under network-instances or unix-socket. If disabled, all gNMI servers will be disabled. timeout : specify the number of seconds the a gNMI connection will remain idle before closing the connection rate-limit : set the max number of connection attempts allowed per minute session-limit : sets the limit on the number of simultaneous active gNMI sessions network-instance : specify the network-instance under which a gNMI server should be reachable admin-state : enabled under the network-instance or unix-socket in order to enable or disable an instance of the gNMI server under a given network-instance use-authentication : specifies whether the RPC should be authenticated against a user using aaa_mgr port : specifies the port on which the gNMI server should listen, by default it will listen on 57400 tls-profile : a TLS profile must be configured, see TLS workshop for details around configuring the profile itself. source-address : specifies an IP address which the gNMI server will listen on, the IP address must be present in the given network-instance. The IP address can be either IPv4 or IPv6. If set to 0.0.0.0 it will listen on any address within the network-instance for IPv4. If set to :: then it will listen on all IPv4 and IPv6 addresses. unix-socket : specifies whether the RPC should be authenticated against a user using aaa_mgr gNMI Examples # Note The gNMIc client is used for each of these examples. gNMIc is one of several open source gNMI clients available. GET Example # demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify get --path \"/system/name/host-name\" Get Response: [ { \"timestamp\" : 1604412647649136431 , \"time\" : \"2020-11-03T15:10:47.649136431+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" , \"values\" : { \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" : \"srl1\" } } ] } ] SET Example # demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify set --replace /system/name/host-name:::string:::test1 Set Response: { \"timestamp\" : 1604412698946981257 , \"time\" : \"2020-11-03T15:11:38.946981257+01:00\" , \"results\" : [ { \"operation\" : \"REPLACE\" , \"path\" : \"system/name/host-name\" } ] } SUBSCRIBE once Example # demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify subscribe --mode once --path \"/system/name/host-name\" { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412764363675969 , \"time\" : \"2020-11-03T15:12:44.363675969+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" , \"values\" : { \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" : \"srl1\" } } ] } SUBSCRIBE on_change Example # demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify subscribe --mode stream --stream-mode on_change --path \"/interface[name=ethernet-1/1]/statistics/in-octets\" { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412831614958694 , \"time\" : \"2020-11-03T15:13:51.614958694+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-interfaces:interface[name=ethernet-1/1]/statistics/in-octets\" , \"values\" : { \"srl_nokia-interfaces:interface/statistics/in-octets\" : \"16170673\" } } ] } { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412833881891643 , \"time\" : \"2020-11-03T15:13:53.881891643+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-interfaces:interface[name=ethernet-1/1]/statistics/in-octets\" , \"values\" : { \"srl_nokia-interfaces:interface/statistics/in-octets\" : \"16170778\" } } ] }","title":"gNMI"},{"location":"mgmt/mgmt-gnmi/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-gnmi/#gnmi","text":"","title":"gNMI"},{"location":"mgmt/mgmt-gnmi/#overview","text":"gRPC Network Management Interface (gNMI) is a gRPC -based protocol that defines a service or set of services (set of RPC methods) used to configure and retrieve data from network devices. The gNMI gRPC service is comprised of four RPCs: CAPABILITIES, GET, SET and SUBSCRIBE. The gNMI spec and associated protobuf defines the type of messages and the data structures used to get, set or stream information to and from network devices. Method Description CAPABILITIES Retrieve the set of capabilities that is supported by the server. This allows the client to validate the service version that is implemented and retrieve the set of models that the server supports. The models can then be specified in subsequent RPCs to restrict the set of data that is utilized. GET Retrieve a snapshot of data from the server. A Get RPC requests that the server snapshots a subset of the data tree as specified by the paths included in the message and serializes this to be returned to the client using the specified encoding. SET Modify the state of data on the server. The paths to modified along with the new values that the client wishes to set the value to. SUBSCRIBE Request the server to send it values of particular paths within the data tree. These values may be streamed at a particular cadence (STREAM), sent one off on a long-lived channel (POLL), or sent as a one-off retrieval (ONCE).","title":"Overview"},{"location":"mgmt/mgmt-gnmi/#gnmi-subscription","text":"A gNMI Subscription to a server consists of the following key attributes: One or multiple paths to subscribe to (what data should be sent to client by server) A subscription mode gNMI SUBSCRIBE RPC defines three subscription modes: ONCE: A subscription to a server is created and a single request is sent. The server creates an update message for the requested path and sends it to the client. The subscription is then terminated. POLL: A subscription to a server is created for a given path, the client will then send a specific message to the server on a timed interval, upon receipt of this message the server will send the data for the path on which the subscription is created. STREAM: A subscription to a server is created for a given path, the server is then expected to send data back to the client for that given path in one of three ways, on_change , on_sample or target_defined","title":"gNMI Subscription"},{"location":"mgmt/mgmt-gnmi/#gnmi-server-configuration","text":"-- { + running } -- [ system gnmi-server ] -- A:srl2# info detail admin-state enable timeout 7200 rate-limit 60 session-limit 20 commit-confirmed-timeout 0 include-defaults-in-config-only-responses false network-instance mgmt { admin-state enable use-authentication true port 57400 tls-profile tls-profile-1 } unix-socket { admin-state disable use-authentication true } Configuration explanation: admin-state : enabled under the global gNMI context it will allow for gNMI to be enabled under network-instances or unix-socket. If disabled, all gNMI servers will be disabled. timeout : specify the number of seconds the a gNMI connection will remain idle before closing the connection rate-limit : set the max number of connection attempts allowed per minute session-limit : sets the limit on the number of simultaneous active gNMI sessions network-instance : specify the network-instance under which a gNMI server should be reachable admin-state : enabled under the network-instance or unix-socket in order to enable or disable an instance of the gNMI server under a given network-instance use-authentication : specifies whether the RPC should be authenticated against a user using aaa_mgr port : specifies the port on which the gNMI server should listen, by default it will listen on 57400 tls-profile : a TLS profile must be configured, see TLS workshop for details around configuring the profile itself. source-address : specifies an IP address which the gNMI server will listen on, the IP address must be present in the given network-instance. The IP address can be either IPv4 or IPv6. If set to 0.0.0.0 it will listen on any address within the network-instance for IPv4. If set to :: then it will listen on all IPv4 and IPv6 addresses. unix-socket : specifies whether the RPC should be authenticated against a user using aaa_mgr","title":"gNMI Server Configuration"},{"location":"mgmt/mgmt-gnmi/#gnmi-examples","text":"Note The gNMIc client is used for each of these examples. gNMIc is one of several open source gNMI clients available.","title":"gNMI Examples"},{"location":"mgmt/mgmt-gnmi/#get-example","text":"demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify get --path \"/system/name/host-name\" Get Response: [ { \"timestamp\" : 1604412647649136431 , \"time\" : \"2020-11-03T15:10:47.649136431+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" , \"values\" : { \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" : \"srl1\" } } ] } ]","title":"GET Example"},{"location":"mgmt/mgmt-gnmi/#set-example","text":"demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify set --replace /system/name/host-name:::string:::test1 Set Response: { \"timestamp\" : 1604412698946981257 , \"time\" : \"2020-11-03T15:11:38.946981257+01:00\" , \"results\" : [ { \"operation\" : \"REPLACE\" , \"path\" : \"system/name/host-name\" } ] }","title":"SET Example"},{"location":"mgmt/mgmt-gnmi/#subscribe-once-example","text":"demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify subscribe --mode once --path \"/system/name/host-name\" { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412764363675969 , \"time\" : \"2020-11-03T15:12:44.363675969+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" , \"values\" : { \"srl_nokia-system:system/srl_nokia-system-name:name/host-name\" : \"srl1\" } } ] }","title":"SUBSCRIBE once Example"},{"location":"mgmt/mgmt-gnmi/#subscribe-on_change-example","text":"demo@demo-sf03:~$ gnmic -a 172 .20.20.2:57400 -e json_ietf -u admin -p admin --skip-verify subscribe --mode stream --stream-mode on_change --path \"/interface[name=ethernet-1/1]/statistics/in-octets\" { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412831614958694 , \"time\" : \"2020-11-03T15:13:51.614958694+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-interfaces:interface[name=ethernet-1/1]/statistics/in-octets\" , \"values\" : { \"srl_nokia-interfaces:interface/statistics/in-octets\" : \"16170673\" } } ] } { \"source\" : \"srl1:57400\" , \"subscription-name\" : \"default\" , \"timestamp\" : 1604412833881891643 , \"time\" : \"2020-11-03T15:13:53.881891643+01:00\" , \"updates\" : [ { \"Path\" : \"srl_nokia-interfaces:interface[name=ethernet-1/1]/statistics/in-octets\" , \"values\" : { \"srl_nokia-interfaces:interface/statistics/in-octets\" : \"16170778\" } } ] }","title":"SUBSCRIBE on_change Example"},{"location":"mgmt/mgmt-intro/","text":"Fabric Management # Introduction # As the networking industry enters the NetOps era, network automation is no longer a nice to have but rather a core requirement. Features like ZTP, continuous monitoring, intent validation and operational changes ( is this new? ) are all now considered tablestakes. The need for network automation has outpaced the capabilities of available networking operating systems and their respective tooling. This has created the need for a slew of network automation tools/frameworks which aim at filling the gaps between the requirements of network operators and the available tools. Nokia\u2019s datacenter portfolio is uniquely positioned to provide best in class tooling, beginning a new, fully modernized network operating system developed with automation at the center of the architecture. This section of the datacenter blueprint will cover several of the key topics for management and automation of the network: Zero Touch Provisioning CLI gNMI Logging","title":"Introduction"},{"location":"mgmt/mgmt-intro/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-intro/#introduction","text":"As the networking industry enters the NetOps era, network automation is no longer a nice to have but rather a core requirement. Features like ZTP, continuous monitoring, intent validation and operational changes ( is this new? ) are all now considered tablestakes. The need for network automation has outpaced the capabilities of available networking operating systems and their respective tooling. This has created the need for a slew of network automation tools/frameworks which aim at filling the gaps between the requirements of network operators and the available tools. Nokia\u2019s datacenter portfolio is uniquely positioned to provide best in class tooling, beginning a new, fully modernized network operating system developed with automation at the center of the architecture. This section of the datacenter blueprint will cover several of the key topics for management and automation of the network: Zero Touch Provisioning CLI gNMI Logging","title":"Introduction"},{"location":"mgmt/mgmt-logging/","text":"Fabric Management # Logging # Overview # Logging capabilities within SR Linux are built on rsyslogd which is a well known and highly utilized Linux logging system which allows for log filtering as well as saving logs to file, memory or sending logs to remote servers. The primary configuration file is /etc/rsyslog.conf and other Linux applications can extend the configuration file by providing their own configuration files in /etc/rsyslog.d/ . SR Linux creates a minimal primary config which enables default Linux logs. SR Linux also creates its own configuration file within /etc/rsyslog.d/ and this file is modified via the SR Linux YANG models (via CLI, gNMI etc). Operators may create their own filters and actions as they see fit however if an operator modifies sections of this file owned by SR Linux, SR Linux will overwrite them with the YANG configs. Log Destinations # SRLinux supports four log destinations: buffer, console, file and remote server. Buffer # In order to reduce disk I/O, a file like log destination can be used called buffer When SR Linux boots it creates a non swapable tmpfs located at /var/log/srlinux/buffer This tmpfs resides entirely in memory and is hard coded to 512MB of RAM When a buffer log destination is configured and committed: SR Linux will calculate the available space based on other buffer retention policies If not, enough space is available the commit will fail Because this is a tmpfs in memory, if the node is rebooted the buffered logs will be lost Buffers can be configured to be persistent: When the persist leaf is enabled the buffered log can be copied to disk The persist value in seconds defines how frequently the log is copied to the disk When a buffer rolls over, it is automatically copied over to the disk Files # As the name suggests log a file log destination will save logs to the disk as a file By default, if no file location is explicitly specified the default file location is /var/log/srlinux/file/<filename> If the default location is changed, it is up the operator to ensure there is enough capacity for the file. Rotate: Number of files to keep in rotation when a maximum file size is reached. Size: Number of bytes an individual output file cannot exceed. Input: Subsystem or facility. Format: Text format of the output syslog messages, in legacy syslog $template style Directory: Fully qualified path of a directory where the log file(s) shall be maintained. Console # Logs will be sent to /dev/console which is usually the console port on the CPM Input: Subsystem or facility Format: Text format of the output syslog messages, in legacy syslog $template style Remote server # Logs can be sent to a remote syslog server A single network-instance can be specified under the /system/logging node to choose which ip-vrf should be used to reach the logging servers Input: Subsystem or facility Remote-port: Transport port for syslog to use for messages sent to a remote server (default syslog port is 514) Transport: Transport protocol for syslog to use for messages sent to a remote server (UDP or TCP) Tracing # SR Linux supports the concept of enabling protocol specific traces. Traces are used to debug a specific protocol, typically on demand during a troubleshooting session. When enabled under a given protocol, traces logs will be available as an input under the corresponding subsystem with a priority of debug . A destination or output of the logs must then be configured to capture the trace logs, setting the input as the corresponding subsystem matching exact debug (for example). As a trace is nothing more than a specific input, all destinations previously discussed are suitable outputs.","title":"Logging"},{"location":"mgmt/mgmt-logging/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-logging/#logging","text":"","title":"Logging"},{"location":"mgmt/mgmt-logging/#overview","text":"Logging capabilities within SR Linux are built on rsyslogd which is a well known and highly utilized Linux logging system which allows for log filtering as well as saving logs to file, memory or sending logs to remote servers. The primary configuration file is /etc/rsyslog.conf and other Linux applications can extend the configuration file by providing their own configuration files in /etc/rsyslog.d/ . SR Linux creates a minimal primary config which enables default Linux logs. SR Linux also creates its own configuration file within /etc/rsyslog.d/ and this file is modified via the SR Linux YANG models (via CLI, gNMI etc). Operators may create their own filters and actions as they see fit however if an operator modifies sections of this file owned by SR Linux, SR Linux will overwrite them with the YANG configs.","title":"Overview"},{"location":"mgmt/mgmt-logging/#log-destinations","text":"SRLinux supports four log destinations: buffer, console, file and remote server.","title":"Log Destinations"},{"location":"mgmt/mgmt-logging/#buffer","text":"In order to reduce disk I/O, a file like log destination can be used called buffer When SR Linux boots it creates a non swapable tmpfs located at /var/log/srlinux/buffer This tmpfs resides entirely in memory and is hard coded to 512MB of RAM When a buffer log destination is configured and committed: SR Linux will calculate the available space based on other buffer retention policies If not, enough space is available the commit will fail Because this is a tmpfs in memory, if the node is rebooted the buffered logs will be lost Buffers can be configured to be persistent: When the persist leaf is enabled the buffered log can be copied to disk The persist value in seconds defines how frequently the log is copied to the disk When a buffer rolls over, it is automatically copied over to the disk","title":"Buffer"},{"location":"mgmt/mgmt-logging/#files","text":"As the name suggests log a file log destination will save logs to the disk as a file By default, if no file location is explicitly specified the default file location is /var/log/srlinux/file/<filename> If the default location is changed, it is up the operator to ensure there is enough capacity for the file. Rotate: Number of files to keep in rotation when a maximum file size is reached. Size: Number of bytes an individual output file cannot exceed. Input: Subsystem or facility. Format: Text format of the output syslog messages, in legacy syslog $template style Directory: Fully qualified path of a directory where the log file(s) shall be maintained.","title":"Files"},{"location":"mgmt/mgmt-logging/#console","text":"Logs will be sent to /dev/console which is usually the console port on the CPM Input: Subsystem or facility Format: Text format of the output syslog messages, in legacy syslog $template style","title":"Console"},{"location":"mgmt/mgmt-logging/#remote-server","text":"Logs can be sent to a remote syslog server A single network-instance can be specified under the /system/logging node to choose which ip-vrf should be used to reach the logging servers Input: Subsystem or facility Remote-port: Transport port for syslog to use for messages sent to a remote server (default syslog port is 514) Transport: Transport protocol for syslog to use for messages sent to a remote server (UDP or TCP)","title":"Remote server"},{"location":"mgmt/mgmt-logging/#tracing","text":"SR Linux supports the concept of enabling protocol specific traces. Traces are used to debug a specific protocol, typically on demand during a troubleshooting session. When enabled under a given protocol, traces logs will be available as an input under the corresponding subsystem with a priority of debug . A destination or output of the logs must then be configured to capture the trace logs, setting the input as the corresponding subsystem matching exact debug (for example). As a trace is nothing more than a specific input, all destinations previously discussed are suitable outputs.","title":"Tracing"},{"location":"mgmt/mgmt-summary/","text":"Fabric Management # Summary # Fabric Management is an important part of ongoing data center fabric operations. It is key that the chosen NOS allow operators maximum flexibility to use automation and operational toolsets of their choice. SR Linux is the only fully modern NOS that is completely model-driven and therefore provides unprecedented opportunity for integration to other systems. This is demonstrated by the consistency of control whether using on-box interfaces like CLI or off-box interfaces such as gNMI.","title":"Summary"},{"location":"mgmt/mgmt-summary/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-summary/#summary","text":"Fabric Management is an important part of ongoing data center fabric operations. It is key that the chosen NOS allow operators maximum flexibility to use automation and operational toolsets of their choice. SR Linux is the only fully modern NOS that is completely model-driven and therefore provides unprecedented opportunity for integration to other systems. This is demonstrated by the consistency of control whether using on-box interfaces like CLI or off-box interfaces such as gNMI.","title":"Summary"},{"location":"mgmt/mgmt-ztp/","text":"Fabric Management # Zero Touch Provisioning # Overview # Zero Touch Provisioning (ZTP) is a mechanism/process used to initialize and configure a device without the need for administrator intervention. A device undergoing ZTP has the ability to be powered on, become operational and remotely configurable without the need for any administrator to pre-provision or pre-configure the device.\u200b In SR Linux, the ZTP process is handled by a ZTP application which is started as a service by the Linux OS.\u200b The ZTP application has a set of actions that can be manually executed via a ZTP CLI by an administrator or executed automatically at boot based on configurable ZTP options.\u200b The option which facilitates the traditional \u201czero touch\u201d aspect of ZTP in SR Linux is called Autoboot. Autoboot is enabled by default from the factory.\u200b Lastly, the ZTP application is also responsible for starting the srlinux service which will start app_mgr and in turn start SR Linux applications.\u200b ZTP Process # This is the ZTP process based on factory-enabled options:\u200b The DUT\u2019s grub bootloader loads the Linux OS from the internal SD card ZTP application is started as service during boot of Linux OS DHCPv4/v6 started on the mgmt interface of the active CPM (chassis serial # sent to DHCP server to identify the DUT) DHCP server provides an IP address as well as the URL of the python provisioning script DUT executes the provisioning script, the script may upgrade the DUT, apply a config, install packages etc. Upon successful completion of provisioning script ZTP starts the srlinux service SR Linux app_mgr starts and begins the application boot sequence Required Components # DHCP server (IPv4 or IPv6) \u2013 To support the assignment of IP addresses through DHCP requests and offers. File server \u2013 for staging and transfer of RPMs, configurations, images, and scripts. HTTP, HTTPS, FTP and TFTP are supported. (For HTTPS, the default Mozilla certificate should be used.) DHCP relay \u2013 required if the server is outside the management interface broadcast domain. Support Deployment Models # Nodes, HTTP file servers, and DHCP server in the same subnet ( Figure 4-1 ) Figure 4-1 HTTP file servers and DHCP server in the same subnet, separate from the nodes ( Figure 4-2 ) Figure 4-2 Nodes, HTTP file servers, and DHCP server in different subnets ( Figure 4-3 ) Figure 4-3 Sample Python Provisioning Script # import errno import os import sys import signal import subprocess from subprocess import Popen , PIPE import threading import commands import time folder = '21.3.1-410' version = '21.3.1-410' srlinux_image_url = 'http://192.168.1.10/load/ {} /srlinux- {} .bin' . format ( folder , version ) srlinux_image_md5_url = 'http://192.168.1.10/load/ {} /srlinux- {} .bin.md5' . format ( folder , version ) srlinux_config_url = 'http://192.168.1.10/configs/ztp/generic_mgmt.json' intf = 'mgmt0' class ProcessError ( Exception ): def __init__ ( self , msg , errno =- 1 ): Exception . __init__ ( self , msg ) self . errno = errno class ProcessOpen ( Popen ): def __init__ ( self , cmd , cwd = None , env = None , flags = None , stdin = None , stdout = None , stderr = None , universal_newlines = True ,): self . __use_killpg = False shell = False if not isinstance ( cmd , ( list , tuple )): shell = True # Set flags to 0, subprocess raises an exception otherwise. flags = 0 # Set a preexec function, this will make the sub-process create it's # own session and process group - bug 80651, bug 85693. preexec_fn = os . setsid self . __cmd = cmd self . __retval = None self . __hasTerminated = threading . Condition () Popen . __init__ ( self , cmd , cwd = cwd , env = env , shell = shell , stdin = stdin , stdout = PIPE , stderr = PIPE , close_fds = True , universal_newlines = universal_newlines , creationflags = flags ,) print ( \"Process [ {} ] pid [ {} ]\" . format ( cmd , self . pid )) def _getReturncode ( self ): return self . __returncode def __finalize ( self ): # Any finalize actions pass def _setReturncode ( self , value ): self . __returncode = value if value is not None : # Notify that the process is done. self . __hasTerminated . acquire () self . __hasTerminated . notifyAll () self . __hasTerminated . release () returncode = property ( fget = _getReturncode , fset = _setReturncode ) def _getRetval ( self ): # Ensure the returncode is set by subprocess if the process is finished. self . poll () return self . returncode retval = property ( fget = _getRetval ) def wait_for ( self , timeout = None ): if timeout is None or timeout < 0 : # Use the parent call. try : out , err = self . communicate () self . __finalize () return self . returncode , out , err except OSError as ex : # If the process has already ended, that is fine. This is # possible when wait is called from a different thread. if ex . errno != 10 : # No child process raise return self . returncode , \"\" , \"\" try : out , err = self . communicate ( timeout = timeout ) self . __finalize () return self . returncode , out , err except subprocess . TimeoutExpired : self . __finalize () raise ProcessError ( \"Process timeout: waited %d seconds, \" \"process not yet finished.\" % ( timeout ) ) def kill ( self , exitCode =- 1 , sig = None ): if sig is None : sig = signal . SIGKILL try : if self . __use_killpg : os . killpg ( self . pid , sig ) else : os . kill ( self . pid , sig ) except OSError as ex : self . __finalize () if ex . errno != 3 : # Ignore: OSError: [Errno 3] No such process raise self . returncode = exitCode self . __finalize () def commandline ( self ): \"\"\"returns string of command line\"\"\" if isinstance ( self . __cmd , six . string ): return self . __cmd return subprocess . list2cmdline ( self . __cmd ) __str__ = commandline def execute_and_out ( command , timeout = None ): print ( \"Executing command: {} \" . format ( command )) process = ProcessOpen ( command ) try : #logger.trace(\"Timeout = {}\".format(timeout)) ret , out , err = process . wait_for ( timeout = timeout ) return ret , out , err except ProcessError : print ( \" {} command timeout\" . format ( command )) process . kill () return errno . ETIMEDOUT , \"\" , \"\" def execute ( command , timeout = None ): ret , _ , _ = execute_and_out ( command , timeout = timeout ) return ret def srlinux (): set_downgrade () nos_install () def set_downgrade (): cmd = 'ztp option downgrade --status enable' ret , out , err = execute_and_out ( cmd ) def post_tasks (): nos_configure () def nos_install (): cmd = 'ztp image upgrade --imageurl {} --md5url {} ' . format ( srlinux_image_url , srlinux_image_md5_url ) ret , out , err = execute_and_out ( cmd ) def nos_configure (): cmd = 'ztp configure-nos --configurl {} ' . format ( srlinux_config_url ) ret , out , err = execute_and_out ( cmd ) def main (): srlinux () # post_tasks() if __name__ == '__main__' : main ()","title":"ZTP"},{"location":"mgmt/mgmt-ztp/#fabric-management","text":"","title":"Fabric Management"},{"location":"mgmt/mgmt-ztp/#zero-touch-provisioning","text":"","title":"Zero Touch Provisioning"},{"location":"mgmt/mgmt-ztp/#overview","text":"Zero Touch Provisioning (ZTP) is a mechanism/process used to initialize and configure a device without the need for administrator intervention. A device undergoing ZTP has the ability to be powered on, become operational and remotely configurable without the need for any administrator to pre-provision or pre-configure the device.\u200b In SR Linux, the ZTP process is handled by a ZTP application which is started as a service by the Linux OS.\u200b The ZTP application has a set of actions that can be manually executed via a ZTP CLI by an administrator or executed automatically at boot based on configurable ZTP options.\u200b The option which facilitates the traditional \u201czero touch\u201d aspect of ZTP in SR Linux is called Autoboot. Autoboot is enabled by default from the factory.\u200b Lastly, the ZTP application is also responsible for starting the srlinux service which will start app_mgr and in turn start SR Linux applications.\u200b","title":"Overview"},{"location":"mgmt/mgmt-ztp/#ztp-process","text":"This is the ZTP process based on factory-enabled options:\u200b The DUT\u2019s grub bootloader loads the Linux OS from the internal SD card ZTP application is started as service during boot of Linux OS DHCPv4/v6 started on the mgmt interface of the active CPM (chassis serial # sent to DHCP server to identify the DUT) DHCP server provides an IP address as well as the URL of the python provisioning script DUT executes the provisioning script, the script may upgrade the DUT, apply a config, install packages etc. Upon successful completion of provisioning script ZTP starts the srlinux service SR Linux app_mgr starts and begins the application boot sequence","title":"ZTP Process"},{"location":"mgmt/mgmt-ztp/#required-components","text":"DHCP server (IPv4 or IPv6) \u2013 To support the assignment of IP addresses through DHCP requests and offers. File server \u2013 for staging and transfer of RPMs, configurations, images, and scripts. HTTP, HTTPS, FTP and TFTP are supported. (For HTTPS, the default Mozilla certificate should be used.) DHCP relay \u2013 required if the server is outside the management interface broadcast domain.","title":"Required Components"},{"location":"mgmt/mgmt-ztp/#support-deployment-models","text":"Nodes, HTTP file servers, and DHCP server in the same subnet ( Figure 4-1 ) Figure 4-1 HTTP file servers and DHCP server in the same subnet, separate from the nodes ( Figure 4-2 ) Figure 4-2 Nodes, HTTP file servers, and DHCP server in different subnets ( Figure 4-3 ) Figure 4-3","title":"Support Deployment Models"},{"location":"mgmt/mgmt-ztp/#sample-python-provisioning-script","text":"import errno import os import sys import signal import subprocess from subprocess import Popen , PIPE import threading import commands import time folder = '21.3.1-410' version = '21.3.1-410' srlinux_image_url = 'http://192.168.1.10/load/ {} /srlinux- {} .bin' . format ( folder , version ) srlinux_image_md5_url = 'http://192.168.1.10/load/ {} /srlinux- {} .bin.md5' . format ( folder , version ) srlinux_config_url = 'http://192.168.1.10/configs/ztp/generic_mgmt.json' intf = 'mgmt0' class ProcessError ( Exception ): def __init__ ( self , msg , errno =- 1 ): Exception . __init__ ( self , msg ) self . errno = errno class ProcessOpen ( Popen ): def __init__ ( self , cmd , cwd = None , env = None , flags = None , stdin = None , stdout = None , stderr = None , universal_newlines = True ,): self . __use_killpg = False shell = False if not isinstance ( cmd , ( list , tuple )): shell = True # Set flags to 0, subprocess raises an exception otherwise. flags = 0 # Set a preexec function, this will make the sub-process create it's # own session and process group - bug 80651, bug 85693. preexec_fn = os . setsid self . __cmd = cmd self . __retval = None self . __hasTerminated = threading . Condition () Popen . __init__ ( self , cmd , cwd = cwd , env = env , shell = shell , stdin = stdin , stdout = PIPE , stderr = PIPE , close_fds = True , universal_newlines = universal_newlines , creationflags = flags ,) print ( \"Process [ {} ] pid [ {} ]\" . format ( cmd , self . pid )) def _getReturncode ( self ): return self . __returncode def __finalize ( self ): # Any finalize actions pass def _setReturncode ( self , value ): self . __returncode = value if value is not None : # Notify that the process is done. self . __hasTerminated . acquire () self . __hasTerminated . notifyAll () self . __hasTerminated . release () returncode = property ( fget = _getReturncode , fset = _setReturncode ) def _getRetval ( self ): # Ensure the returncode is set by subprocess if the process is finished. self . poll () return self . returncode retval = property ( fget = _getRetval ) def wait_for ( self , timeout = None ): if timeout is None or timeout < 0 : # Use the parent call. try : out , err = self . communicate () self . __finalize () return self . returncode , out , err except OSError as ex : # If the process has already ended, that is fine. This is # possible when wait is called from a different thread. if ex . errno != 10 : # No child process raise return self . returncode , \"\" , \"\" try : out , err = self . communicate ( timeout = timeout ) self . __finalize () return self . returncode , out , err except subprocess . TimeoutExpired : self . __finalize () raise ProcessError ( \"Process timeout: waited %d seconds, \" \"process not yet finished.\" % ( timeout ) ) def kill ( self , exitCode =- 1 , sig = None ): if sig is None : sig = signal . SIGKILL try : if self . __use_killpg : os . killpg ( self . pid , sig ) else : os . kill ( self . pid , sig ) except OSError as ex : self . __finalize () if ex . errno != 3 : # Ignore: OSError: [Errno 3] No such process raise self . returncode = exitCode self . __finalize () def commandline ( self ): \"\"\"returns string of command line\"\"\" if isinstance ( self . __cmd , six . string ): return self . __cmd return subprocess . list2cmdline ( self . __cmd ) __str__ = commandline def execute_and_out ( command , timeout = None ): print ( \"Executing command: {} \" . format ( command )) process = ProcessOpen ( command ) try : #logger.trace(\"Timeout = {}\".format(timeout)) ret , out , err = process . wait_for ( timeout = timeout ) return ret , out , err except ProcessError : print ( \" {} command timeout\" . format ( command )) process . kill () return errno . ETIMEDOUT , \"\" , \"\" def execute ( command , timeout = None ): ret , _ , _ = execute_and_out ( command , timeout = timeout ) return ret def srlinux (): set_downgrade () nos_install () def set_downgrade (): cmd = 'ztp option downgrade --status enable' ret , out , err = execute_and_out ( cmd ) def post_tasks (): nos_configure () def nos_install (): cmd = 'ztp image upgrade --imageurl {} --md5url {} ' . format ( srlinux_image_url , srlinux_image_md5_url ) ret , out , err = execute_and_out ( cmd ) def nos_configure (): cmd = 'ztp configure-nos --configurl {} ' . format ( srlinux_config_url ) ret , out , err = execute_and_out ( cmd ) def main (): srlinux () # post_tasks() if __name__ == '__main__' : main ()","title":"Sample Python Provisioning Script"},{"location":"routing/bgp-config/","text":"Fabric Routing # ECMP # Enabling multiple paths simply requires the configuration of ECMP. SR Linux devices allow for up to 128 ECMP paths. For an eBGP underlay, we configure ECMP under the default network instance. See the configuration snippet below for BGP ECMP. --{ runnning }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { ipv4-unicast { multipath { max-paths-level-1 16 max-paths-level-2 16 } } } } } The levels in the above configuration are described as follows: max-paths-level-1 \u2013 next-hop-group is bound directly to a BGP IPv4 prefix max-paths-level-2 \u2013 next-hop-group is bound to the IPv4 or IPv6 route used to resolve a BGP next-hop (for RFC 5599, advertising/receiving IPv4 routes over IPv6 next hops) In SR Linux platforms, the ECMP hash is calculated from the following criteria: hash-seed: value -> 0-65535, configure different values on each router to avoid traffic polarization effects protocol: IPv4 header protocol value or the IPv6 next-header value in the last extension header source-ip: the source IPv4/IPv6 address dest-ip: the destination IPv4/IPv6 address src-port: the source TCP/UDP port number dest-port: the destination TCP/UDP port number We can use the table below to determine hash algorithm calculation for each packet type: Packet Type Hash Calculation IPv4-TCP non-fragment packet hash-seed, source-ip, dest-ip, protocol, TCP src-port, TCP dst-port IPv6-TCP non-fragment packet hash-seed, source-ip, dest-ip, flow-label, protocol, TCP src-port, TCP dst-port IPv4-UDP non-fragment packet hash-seed, source-ip, dest-ip, protocol, UDP src-port, UDP dst-port IPv6-UDP non-fragment packet hash-seed, source-ip, dest-ip, flow-label, protocol, UDP src-port, UDP dst-port eBGP Optimizations - Tuning eBGP for a DC Fabric # As BGP is commonly used for WAN connections, the default timers for path updates and advertisements need to be tuned for a richly connected data center environment to enable faster convergence and updates. Timers and Convergence Optimization # Since a single data center will likely consist of hundreds or even thousands of nodes, it is important to simply the configuration and generalize the best timer values to fit most situations. As previously mentioned, the default timers are typically sufficient for a WAN centric design often utilized by service providers. In the data center, it is important to both maintain stability and match the convergence speed typically seen in link state protocols. Modifying a few main timers will help to achieve this goal. MRAI (Minimum Router Advertisement Interval) # First, let us look at the advertisement interval , which determines how much time must elapse between an advertisement or withdrawal of routes to a BGP peer. The default value of 30s for eBGP peers is not acceptable in a data center environment, so the recommendation is to change this value to 1s. Rather than configuring this for each peer, it can be applied to a peer group and inherited by each peer to which it is applied. --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { timers { minimum-advertisement-interval 1 } } } } } Keepalive and hold timers # By default, the keepalive-interval and hold-time values are 30s and 90s respectively. This means that a BGP peer will send a keepalive message for a session every 30 seconds. If the adjacent peer does not see a keepalive message for 90 seconds, the session will be torn down. In the data center, these values seem like a lifetime. To optimize convergence while also maintaining stability, it is recommend to change the keepalive value to 3 seconds and the hold-down timer to 9 seconds. See the sample configuration below: --{ * candidate shared default }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { timers { hold-time 9 keepalive-interval 3 minimum-advertisement-interval 1 } } } } } The above timers come into play when a BGP peer becomes unreachable. Using an additional protocol such as BFD will allow even faster convergence. BFD (Bidirectional Forwarding Detection) # BFD is a lightweight protocol that can be enabled on a point-to-point link between routers to detect link failures. It is designed to leverage hardware forwarding to enable sub-second failure detection. Once a failure is detected, the link is immediately marked down and upper layer protocols react accordingly (withdraw route, recalculate ECMP, etc.). Because BFD is a protocol unto itself, it can be used in combination with many different routing protocols to assist with faster convergence. In the case of BGP, SR Linux supports BFD for both IPv4 and IPv6 peering. There are two aspects of the BFD configuration. BFD must first be configured for a given subinterface and then associated with a routing protocol. Within BGP, BFD can be configured at the global, group or neighbor level. In this example, BFD is configured on subinterface ethernet-1/2.1 . Timer values are specified in microseconds. The minimum value is 10,000 microseconds (0.01s). --{ running }--[ ]-- # info bfd bfd { subinterface ethernet-1/2.1 { admin-state enable desired-minimum-transmit-interval 250000 required-minimum-receive 250000 detection-multiplier 3 } } Next, BFD is associated with BGP cloud peer group. --{ running }--[ ]-- # info network-instance default protocols bgp network-instance default { protocols { bgp { group cloud { failure-detection { enable-bfd true } } } } } Maintenance Mode # Maintenance mode is a feature that allows you to take a network element out of service so that maintenance actions can be performed such as upgrading the software image. This feature can be applied to other applications like BGP as well. For example, a maintenance group can consist of one or more BGP neighbors and/or peer groups belonging to one or more network instances. Using Maintenance Mode in a Clos data center fabric will allow network operators to take advantage of the multipath capabilities of said architecture while minimizing downtime. If we reference Figure 3-4 , we can apply maintenance mode one of the spines in AS 64550. Figure 3-4 In SR Linux systems, constructs called maintenance groups are used to associate specific objects, such as entire network instances or VRFs, BGP peer groups and neighbors with a maintenance profile. We can create a maintenance profile to specify certain policy changes to a BGP peer group or neighbor. An example policy will prepend additional AS\u2019s in the path to gracefully force traffic to the redundancy spine in AS 64550. When maintenance mode is enabled under the group, the policy is put into effect to cause the desired action. Refer to the Maintenance Mode section of the Nokia SR Linux Configuration Basics guide for more information about configuring maintenance mode. Network Addressing # When designing the network fabric addressing scheme, the most common practice is use private addressing as described in RFC1918 for IPv4 networks and add an additional layer of load balancers and/or border routers to provide the NAT function for communication to external networks. Public IPv4 address space is typically reserved for WAN and Internet connections since that address space is becoming more and more exhausted. IPv6 alleviates the address exhaustion due to the sheer amount of addresses available. It is recommended to manually allocate appropriate address blocks per RFC4193 using a RFC4086 compliant random number generator to improve security. RFC4193 addresses are intended for local communications and are not expected to routable within the public domain. RFC4086 helps to ensure the allocated addresses are resistant to scanning attacks. Avoid manual addressing formats that either mirror IPv4 based addresses or incorporates words. This flexibility with IPv6 addressing, comes at the cost of requiring some type of interopability with IPv4 networks. IPv4 addressing is still the default in today\u2019s networks and some applications may only support IPv4, so a transition is inevitable to reap many of the benefits of IPv6. Addressing Schemes # If we consider the network pictured in Figure 3-3 and use IPv4 addressing for the fabric links the following addressing scheme could be used. The spine nodes and directly connected leaf nodes associated with AS 64550 are \"POD1\" and the respective nodes associated with AS 64560 are \"POD2\". Assuming a contiguous address block is associated with each POD and assuming IPv4 addresses: POD1 links and loopbacks will be part of subnet block 192.168.10.0/28, and POD2 links and loopbacks will part of 192.168.10.16/28. Each block contains enough addresses to carve out /31 subnets and /32 loopbacks required for the 4 node POD. If possible, end hosts connected to the leaf nodes should be assigned addresses out of a subnet that is local to a given leaf. In general, it is best to avoid subnets that span multiple racks. If there is a need for stretching subnets across racks then consider using an overlay virtual network such as EVPN/VXLAN. (Network Virtualization and Overlay networks will be covered in a future section of this data center blueprint.) Advertising IPv4 Routes Using IPv6 Next Hops: RFC5549 # Some data centers are migrating away from a purely IPv4 or dual stack environment to a fully IPv6 fabric. For small to medium size businesses, IPv4 is sufficient to support the number of devices in the network; however, larger enterprises and webscale companies may find themselves running out of IPv4 private address space, hence the need the for IPv6 implementations. In any case, the transition to IPv6 does not occur overnight, so a migration plan is required. Let\u2019s consider two use cases: The network operator has deployed a fully IPv6 data center fabric but must still interop with IPv4 only networks during the transition period. The data center fabric is fully IPv4 but must interop with an IPv6 network. In either case, RFC5549 can solve the need for transition by providing a standard to advertise IPv4 routes using IPv6 next hops. Each interface between BGP peers only needs one or more IPv6 interfaces defined. Figure 3-5 In order to route and forward IPv4 packets over an IPv6 network, the data center node most support the following: The ability to advertise a BGP route for an IPv4 NLRI (Network Layer Reachability Information) with an IPv6 BGP next-hop address. The ability to receive a BGP route for an IPv4 NLRI with an IPv6 BGP next-hop address To enable this behavior, the BGP process must be configured to both advertise and receive IPv4 packets on the IPv6 interfaces, otherwise the packets will be discarded. See the configuration snippet below: --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { ipv4-unicast { advertise-ipv6-next-hops true receive-ipv6-next-hops true } } } } } Route Summarization # Route summarization is one of the techniques that allow routing to scale to support very large networks. For Clos-based networks, however, it is important that route summarization is used carefully. Generally, Clos networks will only summarize routes at the ToR/leaf, especially when a subnet is contrained to a single rack. When deploying many hundreds or thousands of fabric switches, it may be beneficial to summarize at the super spine layer between the \"PODs\". Recall Figure 3-3 and the addressing scheme mentioned above: The spine nodes and directly connected leaf nodes associated with AS 64550 are \"POD1\" and the respective nodes associated with AS 64560 are \"POD2\". Assuming a contiguous address block is associated with each POD and assuming IPv4 addresses: POD1 links and loopbacks will be part of subnet block 192.168.10.0/28, and POD2 links and loopbacks will part of 192.168.10.16/28. Each block contains enough addresses to carve out /31 subnets and /32 loopbacks required for the 4 node POD. The super spines that interconnect POD1 and POD2 can summarize the routes for each POD such that any node in POD1 that needs to reach any node in POD2 will only require any one of the superspine nodes to advertise subnet 192.168.10.16/28. This summarization design will ensure that route tables stay relatively small for underlay routes, and that the spines remain as simple as possible. Routing and route summarization in the case of network virtualization (EVPN/VXLAN overlays) will be considered in a future revision of this document. Route Policy # The route policy for the fabric should kept as simple as possible to allow the policy configuration to remain repeatable throughout the data center fabric. We can implement simple export policies that export all BGP and local routes, and import policies that accept all BGP routes from adjacent BGP peers. Sample export policy: --{ running }--[ ]-- A:srl1# info routing-policy routing-policy { policy export { default-action { reject { } } statement 10 { match { protocol bgp } action { accept { } } } statement 20 { match { protocol local } action { accept { } } } } } Sample Import policy: --{ running }--[ ]-- A:srl1# info routing-policy routing-policy { policy import { default-action { reject { } } statement 10 { match { protocol bgp } action { accept { } } } } } The policies are easily repeatable and can be applied to all data center fabric nodes in the same way, as they do not contain specific IP addresses or other node specific items. The super spine layer will have slightly more complicated policies to allow for route summarization. Unique policies may also be required to tie into the aggregation nodes, those policies is beyond the scope of this document. The fortunate part here is that a specific super spine policy can repeated across all super spine nodes, so there is no need to create unique policy for each device. Dynamic BGP Peering # Dynamic BGP peering is a feature that allows a network operator to simplify BGP configuration for a large number of peers based on specific match criteria. In SRL devices, Dynamic BGP peering is supported for both IPv4 and IPv6 peers. Match criteria can include either IP address blocks, ASN ranges, or both. This feature allows a repeatable configuration across devices, and most useful for automatically connecting downstream devices for spine and super spine devices in a 5-stage Clos architecture. See the configuration snippet below for example of matching on IP and ASN criteria: --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { dynamic-neighbors { accept { match 192.168.10.0/24 { peer-group spine allowed-peer-as [ 64550 ] } } } } } } The above configuration could be applied to the super spine for example to automatically peer with all spine switches in specific pod.","title":"BGP Configuration"},{"location":"routing/bgp-config/#fabric-routing","text":"","title":"Fabric Routing"},{"location":"routing/bgp-config/#ecmp","text":"Enabling multiple paths simply requires the configuration of ECMP. SR Linux devices allow for up to 128 ECMP paths. For an eBGP underlay, we configure ECMP under the default network instance. See the configuration snippet below for BGP ECMP. --{ runnning }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { ipv4-unicast { multipath { max-paths-level-1 16 max-paths-level-2 16 } } } } } The levels in the above configuration are described as follows: max-paths-level-1 \u2013 next-hop-group is bound directly to a BGP IPv4 prefix max-paths-level-2 \u2013 next-hop-group is bound to the IPv4 or IPv6 route used to resolve a BGP next-hop (for RFC 5599, advertising/receiving IPv4 routes over IPv6 next hops) In SR Linux platforms, the ECMP hash is calculated from the following criteria: hash-seed: value -> 0-65535, configure different values on each router to avoid traffic polarization effects protocol: IPv4 header protocol value or the IPv6 next-header value in the last extension header source-ip: the source IPv4/IPv6 address dest-ip: the destination IPv4/IPv6 address src-port: the source TCP/UDP port number dest-port: the destination TCP/UDP port number We can use the table below to determine hash algorithm calculation for each packet type: Packet Type Hash Calculation IPv4-TCP non-fragment packet hash-seed, source-ip, dest-ip, protocol, TCP src-port, TCP dst-port IPv6-TCP non-fragment packet hash-seed, source-ip, dest-ip, flow-label, protocol, TCP src-port, TCP dst-port IPv4-UDP non-fragment packet hash-seed, source-ip, dest-ip, protocol, UDP src-port, UDP dst-port IPv6-UDP non-fragment packet hash-seed, source-ip, dest-ip, flow-label, protocol, UDP src-port, UDP dst-port","title":"ECMP"},{"location":"routing/bgp-config/#ebgp-optimizations-tuning-ebgp-for-a-dc-fabric","text":"As BGP is commonly used for WAN connections, the default timers for path updates and advertisements need to be tuned for a richly connected data center environment to enable faster convergence and updates.","title":"eBGP Optimizations - Tuning eBGP for a DC Fabric"},{"location":"routing/bgp-config/#timers-and-convergence-optimization","text":"Since a single data center will likely consist of hundreds or even thousands of nodes, it is important to simply the configuration and generalize the best timer values to fit most situations. As previously mentioned, the default timers are typically sufficient for a WAN centric design often utilized by service providers. In the data center, it is important to both maintain stability and match the convergence speed typically seen in link state protocols. Modifying a few main timers will help to achieve this goal.","title":"Timers and Convergence Optimization"},{"location":"routing/bgp-config/#mrai-minimum-router-advertisement-interval","text":"First, let us look at the advertisement interval , which determines how much time must elapse between an advertisement or withdrawal of routes to a BGP peer. The default value of 30s for eBGP peers is not acceptable in a data center environment, so the recommendation is to change this value to 1s. Rather than configuring this for each peer, it can be applied to a peer group and inherited by each peer to which it is applied. --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { timers { minimum-advertisement-interval 1 } } } } }","title":"MRAI (Minimum Router Advertisement Interval)"},{"location":"routing/bgp-config/#keepalive-and-hold-timers","text":"By default, the keepalive-interval and hold-time values are 30s and 90s respectively. This means that a BGP peer will send a keepalive message for a session every 30 seconds. If the adjacent peer does not see a keepalive message for 90 seconds, the session will be torn down. In the data center, these values seem like a lifetime. To optimize convergence while also maintaining stability, it is recommend to change the keepalive value to 3 seconds and the hold-down timer to 9 seconds. See the sample configuration below: --{ * candidate shared default }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { timers { hold-time 9 keepalive-interval 3 minimum-advertisement-interval 1 } } } } } The above timers come into play when a BGP peer becomes unreachable. Using an additional protocol such as BFD will allow even faster convergence.","title":"Keepalive and hold timers"},{"location":"routing/bgp-config/#bfd-bidirectional-forwarding-detection","text":"BFD is a lightweight protocol that can be enabled on a point-to-point link between routers to detect link failures. It is designed to leverage hardware forwarding to enable sub-second failure detection. Once a failure is detected, the link is immediately marked down and upper layer protocols react accordingly (withdraw route, recalculate ECMP, etc.). Because BFD is a protocol unto itself, it can be used in combination with many different routing protocols to assist with faster convergence. In the case of BGP, SR Linux supports BFD for both IPv4 and IPv6 peering. There are two aspects of the BFD configuration. BFD must first be configured for a given subinterface and then associated with a routing protocol. Within BGP, BFD can be configured at the global, group or neighbor level. In this example, BFD is configured on subinterface ethernet-1/2.1 . Timer values are specified in microseconds. The minimum value is 10,000 microseconds (0.01s). --{ running }--[ ]-- # info bfd bfd { subinterface ethernet-1/2.1 { admin-state enable desired-minimum-transmit-interval 250000 required-minimum-receive 250000 detection-multiplier 3 } } Next, BFD is associated with BGP cloud peer group. --{ running }--[ ]-- # info network-instance default protocols bgp network-instance default { protocols { bgp { group cloud { failure-detection { enable-bfd true } } } } }","title":"BFD (Bidirectional Forwarding Detection)"},{"location":"routing/bgp-config/#maintenance-mode","text":"Maintenance mode is a feature that allows you to take a network element out of service so that maintenance actions can be performed such as upgrading the software image. This feature can be applied to other applications like BGP as well. For example, a maintenance group can consist of one or more BGP neighbors and/or peer groups belonging to one or more network instances. Using Maintenance Mode in a Clos data center fabric will allow network operators to take advantage of the multipath capabilities of said architecture while minimizing downtime. If we reference Figure 3-4 , we can apply maintenance mode one of the spines in AS 64550. Figure 3-4 In SR Linux systems, constructs called maintenance groups are used to associate specific objects, such as entire network instances or VRFs, BGP peer groups and neighbors with a maintenance profile. We can create a maintenance profile to specify certain policy changes to a BGP peer group or neighbor. An example policy will prepend additional AS\u2019s in the path to gracefully force traffic to the redundancy spine in AS 64550. When maintenance mode is enabled under the group, the policy is put into effect to cause the desired action. Refer to the Maintenance Mode section of the Nokia SR Linux Configuration Basics guide for more information about configuring maintenance mode.","title":"Maintenance Mode"},{"location":"routing/bgp-config/#network-addressing","text":"When designing the network fabric addressing scheme, the most common practice is use private addressing as described in RFC1918 for IPv4 networks and add an additional layer of load balancers and/or border routers to provide the NAT function for communication to external networks. Public IPv4 address space is typically reserved for WAN and Internet connections since that address space is becoming more and more exhausted. IPv6 alleviates the address exhaustion due to the sheer amount of addresses available. It is recommended to manually allocate appropriate address blocks per RFC4193 using a RFC4086 compliant random number generator to improve security. RFC4193 addresses are intended for local communications and are not expected to routable within the public domain. RFC4086 helps to ensure the allocated addresses are resistant to scanning attacks. Avoid manual addressing formats that either mirror IPv4 based addresses or incorporates words. This flexibility with IPv6 addressing, comes at the cost of requiring some type of interopability with IPv4 networks. IPv4 addressing is still the default in today\u2019s networks and some applications may only support IPv4, so a transition is inevitable to reap many of the benefits of IPv6.","title":"Network Addressing"},{"location":"routing/bgp-config/#addressing-schemes","text":"If we consider the network pictured in Figure 3-3 and use IPv4 addressing for the fabric links the following addressing scheme could be used. The spine nodes and directly connected leaf nodes associated with AS 64550 are \"POD1\" and the respective nodes associated with AS 64560 are \"POD2\". Assuming a contiguous address block is associated with each POD and assuming IPv4 addresses: POD1 links and loopbacks will be part of subnet block 192.168.10.0/28, and POD2 links and loopbacks will part of 192.168.10.16/28. Each block contains enough addresses to carve out /31 subnets and /32 loopbacks required for the 4 node POD. If possible, end hosts connected to the leaf nodes should be assigned addresses out of a subnet that is local to a given leaf. In general, it is best to avoid subnets that span multiple racks. If there is a need for stretching subnets across racks then consider using an overlay virtual network such as EVPN/VXLAN. (Network Virtualization and Overlay networks will be covered in a future section of this data center blueprint.)","title":"Addressing Schemes"},{"location":"routing/bgp-config/#advertising-ipv4-routes-using-ipv6-next-hops-rfc5549","text":"Some data centers are migrating away from a purely IPv4 or dual stack environment to a fully IPv6 fabric. For small to medium size businesses, IPv4 is sufficient to support the number of devices in the network; however, larger enterprises and webscale companies may find themselves running out of IPv4 private address space, hence the need the for IPv6 implementations. In any case, the transition to IPv6 does not occur overnight, so a migration plan is required. Let\u2019s consider two use cases: The network operator has deployed a fully IPv6 data center fabric but must still interop with IPv4 only networks during the transition period. The data center fabric is fully IPv4 but must interop with an IPv6 network. In either case, RFC5549 can solve the need for transition by providing a standard to advertise IPv4 routes using IPv6 next hops. Each interface between BGP peers only needs one or more IPv6 interfaces defined. Figure 3-5 In order to route and forward IPv4 packets over an IPv6 network, the data center node most support the following: The ability to advertise a BGP route for an IPv4 NLRI (Network Layer Reachability Information) with an IPv6 BGP next-hop address. The ability to receive a BGP route for an IPv4 NLRI with an IPv6 BGP next-hop address To enable this behavior, the BGP process must be configured to both advertise and receive IPv4 packets on the IPv6 interfaces, otherwise the packets will be discarded. See the configuration snippet below: --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { group cloud { ipv4-unicast { advertise-ipv6-next-hops true receive-ipv6-next-hops true } } } } }","title":"Advertising IPv4 Routes Using IPv6 Next Hops: RFC5549"},{"location":"routing/bgp-config/#route-summarization","text":"Route summarization is one of the techniques that allow routing to scale to support very large networks. For Clos-based networks, however, it is important that route summarization is used carefully. Generally, Clos networks will only summarize routes at the ToR/leaf, especially when a subnet is contrained to a single rack. When deploying many hundreds or thousands of fabric switches, it may be beneficial to summarize at the super spine layer between the \"PODs\". Recall Figure 3-3 and the addressing scheme mentioned above: The spine nodes and directly connected leaf nodes associated with AS 64550 are \"POD1\" and the respective nodes associated with AS 64560 are \"POD2\". Assuming a contiguous address block is associated with each POD and assuming IPv4 addresses: POD1 links and loopbacks will be part of subnet block 192.168.10.0/28, and POD2 links and loopbacks will part of 192.168.10.16/28. Each block contains enough addresses to carve out /31 subnets and /32 loopbacks required for the 4 node POD. The super spines that interconnect POD1 and POD2 can summarize the routes for each POD such that any node in POD1 that needs to reach any node in POD2 will only require any one of the superspine nodes to advertise subnet 192.168.10.16/28. This summarization design will ensure that route tables stay relatively small for underlay routes, and that the spines remain as simple as possible. Routing and route summarization in the case of network virtualization (EVPN/VXLAN overlays) will be considered in a future revision of this document.","title":"Route Summarization"},{"location":"routing/bgp-config/#route-policy","text":"The route policy for the fabric should kept as simple as possible to allow the policy configuration to remain repeatable throughout the data center fabric. We can implement simple export policies that export all BGP and local routes, and import policies that accept all BGP routes from adjacent BGP peers. Sample export policy: --{ running }--[ ]-- A:srl1# info routing-policy routing-policy { policy export { default-action { reject { } } statement 10 { match { protocol bgp } action { accept { } } } statement 20 { match { protocol local } action { accept { } } } } } Sample Import policy: --{ running }--[ ]-- A:srl1# info routing-policy routing-policy { policy import { default-action { reject { } } statement 10 { match { protocol bgp } action { accept { } } } } } The policies are easily repeatable and can be applied to all data center fabric nodes in the same way, as they do not contain specific IP addresses or other node specific items. The super spine layer will have slightly more complicated policies to allow for route summarization. Unique policies may also be required to tie into the aggregation nodes, those policies is beyond the scope of this document. The fortunate part here is that a specific super spine policy can repeated across all super spine nodes, so there is no need to create unique policy for each device.","title":"Route Policy"},{"location":"routing/bgp-config/#dynamic-bgp-peering","text":"Dynamic BGP peering is a feature that allows a network operator to simplify BGP configuration for a large number of peers based on specific match criteria. In SRL devices, Dynamic BGP peering is supported for both IPv4 and IPv6 peers. Match criteria can include either IP address blocks, ASN ranges, or both. This feature allows a repeatable configuration across devices, and most useful for automatically connecting downstream devices for spine and super spine devices in a 5-stage Clos architecture. See the configuration snippet below for example of matching on IP and ASN criteria: --{ running }--[ ]-- A:srl1# info network-instance default network-instance default { protocols { bgp { dynamic-neighbors { accept { match 192.168.10.0/24 { peer-group spine allowed-peer-as [ 64550 ] } } } } } } The above configuration could be applied to the super spine for example to automatically peer with all spine switches in specific pod.","title":"Dynamic BGP Peering"},{"location":"routing/bgp-design/","text":"Fabric Routing # BGP Design # eBGP Considerations # For the remainder of the data center BGP implementation discussion, we will use a common 5-stage / 3-tier Clos architecture as our reference design ( Figure 3-2 ). This design is quite popular among large network operators across industries. The data center edge routers in this design are meant for north/south traffic that needs to exit the data center. East-West traffic between nodes in the same data center flows across the spine and/or superspine layer. Figure 3-2 Autonomous System Selection # There is some flexibility in determining the autonomous system numbering scheme and assignment, but the network designer will need to work within the constraints of eBGP and follow some best practices for optimal path selection. All best-path-selection criteria being equal, AS path will be the tie breaker for determining the route to be installed into the forwarding table. It is also necessary to consider scale when determining whether to choose 2-byte or 4-byte AS numbering. 2-byte or 4-byte Private AS? # When determining ASN assignment, the best practice is to utilize private ASNs (2- or 4-byte) to avoid the chance of leaking a public (usually 2-byte) ASN to an external network which could potentially cause havoc to internet peers. 2-byte private ASNs provide a total 1023 unique numbers in the range of 64512-65534. For smaller Data Centers, this number may enough and there are mechanisms that facilitate the reuse of ASNs within a DC if this number falls short. For larger data centers and those considering extending BGP to the host (a topic that will be discussed later), 4-byte ASNs may prove more appropriate. 4-byte private ASNs provide a total of 95 million unique numbers in the range of 4200000000\u20134294967294. Nearly every vendor supports 4-byte ASNs, with only possible downside being the supported notation for vendor interop. SR Linux devices support ASplain notation only. Once the decision regarding 2-byte vs 4-byte ASNs has been made, assignment of these numbers to each router within the Clos fabric must be considered. Logically, it would make sense to assign a unique AS to each and every node within the fabric, but there are downsides to this decision due to the nature of path vector protocols like BGP. AS Path Hunting, Multipathing and AS Assignment # One of the differences between BGP and a link state protocol like OSPF or IS-IS is the lack of awareness of the overall network topology. An eBGP speaker only knows about its directly connected neighbors. If each router in the fabric is assigned a unique AS, \"path hunting\" becomes a potential issue that can lead to a \"hunt-to-infinity\" scenario. AS path hunting behavior means that when a route is withdrawn, BGP does not know if the route has truly disappeared or if there is another possible path. BGP will keep \"hunting\" all other possible paths until the prefix disappears from all routing tables. This scenario can lead to long convergence times and is not desirable for a data center fabric. An alternative approach to assigning each node its own AS is to use the following scheme (see Figure 3-3 ): Assign a unique AS per Leaf (ToR) Assign a common AS per Spine per pod Assign a common AS for the Super Spine Figure 3-3 In this example we follow the above recommended AS assignments to minimize path hunting and allow for better multipathing. Let\u2019s walk through a quick example: In Figure 3-3 , the ToR with AS 64562 will advertise its locally learned routes. Spines in AS 64560 will advertise that route via the super spine in AS64600, down to the spines in AS64550, and finally to their Directly connected ToRs. When ToR with AS64551 receives the route, it will see the following AS path from both spine routers: (64562, 64560, 64600, 64550). The ToR will see the same AS_PATH from each spine router (each with a different next-hop) and will install both routes in its FIB. ECMP will see two routes and make use of both. If the AS_PATH was different, BGP would go further down the list of the best path selection algorithm and only install one route.","title":"BGP Design"},{"location":"routing/bgp-design/#fabric-routing","text":"","title":"Fabric Routing"},{"location":"routing/bgp-design/#bgp-design","text":"","title":"BGP Design"},{"location":"routing/bgp-design/#ebgp-considerations","text":"For the remainder of the data center BGP implementation discussion, we will use a common 5-stage / 3-tier Clos architecture as our reference design ( Figure 3-2 ). This design is quite popular among large network operators across industries. The data center edge routers in this design are meant for north/south traffic that needs to exit the data center. East-West traffic between nodes in the same data center flows across the spine and/or superspine layer. Figure 3-2","title":"eBGP Considerations"},{"location":"routing/bgp-design/#autonomous-system-selection","text":"There is some flexibility in determining the autonomous system numbering scheme and assignment, but the network designer will need to work within the constraints of eBGP and follow some best practices for optimal path selection. All best-path-selection criteria being equal, AS path will be the tie breaker for determining the route to be installed into the forwarding table. It is also necessary to consider scale when determining whether to choose 2-byte or 4-byte AS numbering.","title":"Autonomous System Selection"},{"location":"routing/bgp-design/#2-byte-or-4-byte-private-as","text":"When determining ASN assignment, the best practice is to utilize private ASNs (2- or 4-byte) to avoid the chance of leaking a public (usually 2-byte) ASN to an external network which could potentially cause havoc to internet peers. 2-byte private ASNs provide a total 1023 unique numbers in the range of 64512-65534. For smaller Data Centers, this number may enough and there are mechanisms that facilitate the reuse of ASNs within a DC if this number falls short. For larger data centers and those considering extending BGP to the host (a topic that will be discussed later), 4-byte ASNs may prove more appropriate. 4-byte private ASNs provide a total of 95 million unique numbers in the range of 4200000000\u20134294967294. Nearly every vendor supports 4-byte ASNs, with only possible downside being the supported notation for vendor interop. SR Linux devices support ASplain notation only. Once the decision regarding 2-byte vs 4-byte ASNs has been made, assignment of these numbers to each router within the Clos fabric must be considered. Logically, it would make sense to assign a unique AS to each and every node within the fabric, but there are downsides to this decision due to the nature of path vector protocols like BGP.","title":"2-byte or 4-byte Private AS?"},{"location":"routing/bgp-design/#as-path-hunting-multipathing-and-as-assignment","text":"One of the differences between BGP and a link state protocol like OSPF or IS-IS is the lack of awareness of the overall network topology. An eBGP speaker only knows about its directly connected neighbors. If each router in the fabric is assigned a unique AS, \"path hunting\" becomes a potential issue that can lead to a \"hunt-to-infinity\" scenario. AS path hunting behavior means that when a route is withdrawn, BGP does not know if the route has truly disappeared or if there is another possible path. BGP will keep \"hunting\" all other possible paths until the prefix disappears from all routing tables. This scenario can lead to long convergence times and is not desirable for a data center fabric. An alternative approach to assigning each node its own AS is to use the following scheme (see Figure 3-3 ): Assign a unique AS per Leaf (ToR) Assign a common AS per Spine per pod Assign a common AS for the Super Spine Figure 3-3 In this example we follow the above recommended AS assignments to minimize path hunting and allow for better multipathing. Let\u2019s walk through a quick example: In Figure 3-3 , the ToR with AS 64562 will advertise its locally learned routes. Spines in AS 64560 will advertise that route via the super spine in AS64600, down to the spines in AS64550, and finally to their Directly connected ToRs. When ToR with AS64551 receives the route, it will see the following AS path from both spine routers: (64562, 64560, 64600, 64550). The ToR will see the same AS_PATH from each spine router (each with a different next-hop) and will install both routes in its FIB. ECMP will see two routes and make use of both. If the AS_PATH was different, BGP would go further down the list of the best path selection algorithm and only install one route.","title":"AS Path Hunting, Multipathing and AS Assignment"},{"location":"routing/protocols/","text":"Fabric Routing # Protocol Selection # Most organizations already use several different routing protocols so what protocol is best for a data center fabric? This topic has been covered by many different network operators and vendors over the last few years. Routing Design for Large Scale Data Centers: BGP is a Better IGP and RFC 7938 - Use of BGP for Routing in Large-Scale Data Centers The concensus choice for a data center routing protocol is to use BGP and specifically eBGP. (Certainly other protocols, such as OSPF or IS-IS, could be used and are supported by SR Linux but the focus of the document will be on BGP.) BGP lends itself well to the data center use case for several key reasons: Multi-hop Multipathing Redundancy and Resiliency Scalability Multiprotocol Support Simplicity Operations Let's look at each of these in more detail. Multi-hop Multipathing # Forwarding traffic over many available paths is a fundamental requirement for a Clos-based fabric for both throughput and resiliency. Routing protocols implement Equal-Cost Multipath (ECMP) to forward traffic over many paths (often 64 or more) assuming all paths are \"equal.\" Examples of how ECMP is configured in SR Linux will be covered later in this document. Unlike link state protocols, BGP supports ECMP routes that consist of multiple hops. In case of eBGP, this path is tracked in the AS Path attribute, and multi-hop loops are prevented through the BGP route acceptance algorithms. Redundancy and Resiliency # The data center fabric must have the ability to minimize the effect of losing a single link or node and provide fast re-convergence. Consider a simple spine-leaf data center fabric in Figure 3-1 . Figure 3-1 Each leaf in the design has 4 uplinks, with each uplink connecting to a different spine in the fabric. With multipathing enabled each uplink link is utilized under normal condititions. Losing a single spine switch will result in a 25% loss in traffic forwarding capability. The same principles can be applied to scheduled maintenance. For example, each spine in Pod1 can be taken down systematically for an upgrade without interrupting service for the entire Pod. Depending on the fabric design and tolerance for loss, operators can rely on fast failover mechanisms to re-route traffic with minimal traffic loss. Alternatively, by updating the BGP policies on the node in maintenance, traffic can be gracefully diverted without loss of in-flight packets. Once the ECMP route through the node in maintenance has been withdrawn, traffic will be redistributed by ECMP over the remaining links between leaf and spine. Scalability # Clos-based networks can scale to support hundreds or even thousands of racks using mutliple tiers (or stages) as discussed in the previous chapter. In large networks, it is critical that the chosen routing protocol can also scale to hundreds or thousands of routers. Path-vector routing protocols like BGP are primarily concerned about maintaining state with their peer routes as opposed to link-state protocols such as OSPF which require understanding of the entire network. This behavior provides BGP an advantage in large networks. As mentioned before, OSPF or IS-IS could be used to build data center networks but introduce extra complexity for very large scale deployments. (Detailing of these differences is beyond the scope of this project, but please reach out if you would like to discuss further.) Using eBGP specifically, along with well-designed use of private AS numbering, makes BGP particularly well suited to large scale deployments. These considerations will be discussed in more detail in the BGP Configuration section. Multiprotocol Support # Modern data center networks often require both IPv4 and IPv6 support and perhaps even EVPN or MPLS for virtual networks. Multiprotocol support is a key strength of BGP. IPv4, IPv6, EVPN and more can be supported in a single peering session through a feature known as a network address family . RFC5549 leverages these capabilities of BGP and outlines the ability to support \"IPv4 over IPv6\". This RFC and its applicability will be discussed later in this guide. Simplicity # Simplicity is an important part of operating networks at scale. If a single protocol can be used instead of two or even three that has value. Modern data center networks that host applications running on Kubernetes for example will often recommend running BGP directly on servers and routing all the way to the end host. This brings some benefits but it also requires administration of a routing protocol on the end host. Being able to use the same protocol (BGP in this case) makes interoperability, configuration and operations much easier. Operations # Once understood, BGP is a simple and straightforward protocol. It presents a clear and easy to understand routing table. Because it is a path vector protocol, when combined with well designed AS numbering it becomes quite easy to troubleshoot by looking at the AS_PATH attribute. Lastly, because BGP is an open standard there are many good implementations that are able to interoperate and provide choice for end users.","title":"Protocol Selection"},{"location":"routing/protocols/#fabric-routing","text":"","title":"Fabric Routing"},{"location":"routing/protocols/#protocol-selection","text":"Most organizations already use several different routing protocols so what protocol is best for a data center fabric? This topic has been covered by many different network operators and vendors over the last few years. Routing Design for Large Scale Data Centers: BGP is a Better IGP and RFC 7938 - Use of BGP for Routing in Large-Scale Data Centers The concensus choice for a data center routing protocol is to use BGP and specifically eBGP. (Certainly other protocols, such as OSPF or IS-IS, could be used and are supported by SR Linux but the focus of the document will be on BGP.) BGP lends itself well to the data center use case for several key reasons: Multi-hop Multipathing Redundancy and Resiliency Scalability Multiprotocol Support Simplicity Operations Let's look at each of these in more detail.","title":"Protocol Selection"},{"location":"routing/protocols/#multi-hop-multipathing","text":"Forwarding traffic over many available paths is a fundamental requirement for a Clos-based fabric for both throughput and resiliency. Routing protocols implement Equal-Cost Multipath (ECMP) to forward traffic over many paths (often 64 or more) assuming all paths are \"equal.\" Examples of how ECMP is configured in SR Linux will be covered later in this document. Unlike link state protocols, BGP supports ECMP routes that consist of multiple hops. In case of eBGP, this path is tracked in the AS Path attribute, and multi-hop loops are prevented through the BGP route acceptance algorithms.","title":"Multi-hop Multipathing"},{"location":"routing/protocols/#redundancy-and-resiliency","text":"The data center fabric must have the ability to minimize the effect of losing a single link or node and provide fast re-convergence. Consider a simple spine-leaf data center fabric in Figure 3-1 . Figure 3-1 Each leaf in the design has 4 uplinks, with each uplink connecting to a different spine in the fabric. With multipathing enabled each uplink link is utilized under normal condititions. Losing a single spine switch will result in a 25% loss in traffic forwarding capability. The same principles can be applied to scheduled maintenance. For example, each spine in Pod1 can be taken down systematically for an upgrade without interrupting service for the entire Pod. Depending on the fabric design and tolerance for loss, operators can rely on fast failover mechanisms to re-route traffic with minimal traffic loss. Alternatively, by updating the BGP policies on the node in maintenance, traffic can be gracefully diverted without loss of in-flight packets. Once the ECMP route through the node in maintenance has been withdrawn, traffic will be redistributed by ECMP over the remaining links between leaf and spine.","title":"Redundancy and Resiliency"},{"location":"routing/protocols/#scalability","text":"Clos-based networks can scale to support hundreds or even thousands of racks using mutliple tiers (or stages) as discussed in the previous chapter. In large networks, it is critical that the chosen routing protocol can also scale to hundreds or thousands of routers. Path-vector routing protocols like BGP are primarily concerned about maintaining state with their peer routes as opposed to link-state protocols such as OSPF which require understanding of the entire network. This behavior provides BGP an advantage in large networks. As mentioned before, OSPF or IS-IS could be used to build data center networks but introduce extra complexity for very large scale deployments. (Detailing of these differences is beyond the scope of this project, but please reach out if you would like to discuss further.) Using eBGP specifically, along with well-designed use of private AS numbering, makes BGP particularly well suited to large scale deployments. These considerations will be discussed in more detail in the BGP Configuration section.","title":"Scalability"},{"location":"routing/protocols/#multiprotocol-support","text":"Modern data center networks often require both IPv4 and IPv6 support and perhaps even EVPN or MPLS for virtual networks. Multiprotocol support is a key strength of BGP. IPv4, IPv6, EVPN and more can be supported in a single peering session through a feature known as a network address family . RFC5549 leverages these capabilities of BGP and outlines the ability to support \"IPv4 over IPv6\". This RFC and its applicability will be discussed later in this guide.","title":"Multiprotocol Support"},{"location":"routing/protocols/#simplicity","text":"Simplicity is an important part of operating networks at scale. If a single protocol can be used instead of two or even three that has value. Modern data center networks that host applications running on Kubernetes for example will often recommend running BGP directly on servers and routing all the way to the end host. This brings some benefits but it also requires administration of a routing protocol on the end host. Being able to use the same protocol (BGP in this case) makes interoperability, configuration and operations much easier.","title":"Simplicity"},{"location":"routing/protocols/#operations","text":"Once understood, BGP is a simple and straightforward protocol. It presents a clear and easy to understand routing table. Because it is a path vector protocol, when combined with well designed AS numbering it becomes quite easy to troubleshoot by looking at the AS_PATH attribute. Lastly, because BGP is an open standard there are many good implementations that are able to interoperate and provide choice for end users.","title":"Operations"},{"location":"routing/routing-intro/","text":"Fabric Routing # Introduction # Moving from switching/bridging to routing is a key change when implementing a Clos-based network fabric as described in the previous section. By design, a Clos fabric provides many links between different stages of the network. Routing protocols are designed to use many paths while maintaining a resilient, loop-free network. This section will review considerations for selecting a fabric routing protocol and why Nokia recommends using BGP.","title":"Introduction"},{"location":"routing/routing-intro/#fabric-routing","text":"","title":"Fabric Routing"},{"location":"routing/routing-intro/#introduction","text":"Moving from switching/bridging to routing is a key change when implementing a Clos-based network fabric as described in the previous section. By design, a Clos fabric provides many links between different stages of the network. Routing protocols are designed to use many paths while maintaining a resilient, loop-free network. This section will review considerations for selecting a fabric routing protocol and why Nokia recommends using BGP.","title":"Introduction"},{"location":"routing/summary/","text":"Fabric Routing # Summary # Clos-based network fabrics have proven to be a robust yet cost-effective design choice for the physical topology. This section of the Nokia data center blueprint has attempted to present the main considerations for choosing the routing protocol best suited to your data center fabric. Understanding the applications that will be deployed on the network will help to determine which protocol is best. Do any of the applications require IPv4 or IPv6? Will routing be extended all the way to the end host (for something like Kubernetes)? The expected size of the network is another important factor. How many switches will be deployed? How many links will connect each tier of the network? Are any routing policies needed (often this is only needed at the boundary between networks)? Using the answers to these questions, decisions can be made about IP addressing, AS numbering and so forth. In most cases, it is recommended to build the data center fabric using eBGP and incorporating the best practices outlined in this document. Our next section will cover the configuration, management and operations of a data center fabric.","title":"Summary"},{"location":"routing/summary/#fabric-routing","text":"","title":"Fabric Routing"},{"location":"routing/summary/#summary","text":"Clos-based network fabrics have proven to be a robust yet cost-effective design choice for the physical topology. This section of the Nokia data center blueprint has attempted to present the main considerations for choosing the routing protocol best suited to your data center fabric. Understanding the applications that will be deployed on the network will help to determine which protocol is best. Do any of the applications require IPv4 or IPv6? Will routing be extended all the way to the end host (for something like Kubernetes)? The expected size of the network is another important factor. How many switches will be deployed? How many links will connect each tier of the network? Are any routing policies needed (often this is only needed at the boundary between networks)? Using the answers to these questions, decisions can be made about IP addressing, AS numbering and so forth. In most cases, it is recommended to build the data center fabric using eBGP and incorporating the best practices outlined in this document. Our next section will cover the configuration, management and operations of a data center fabric.","title":"Summary"}]}